from pyspark.sql import SparkSession
from pyspark.sql.functions import col, split, upper, expr
import yaml
import time
import sys

# Initialize Spark session
spark = SparkSession.builder \
    .appName("Census Update") \
    .config("spark.jars", "/path/to/postgresql-42.2.18.jar") \
    .getOrCreate()

# Load states from YAML file
with open("config.yaml", 'r') as file:
    config = yaml.safe_load(file)
states_to_update = config['states_to_update']

batch_size = 100000  # Define your batch size
total_updates = 0

# JDBC connection properties
jdbc_url = "jdbc:postgresql://your-database-url:5432/yourdbname"
connection_properties = {
    "user": "yourusername",
    "password": "yourpassword",
    "driver": "org.postgresql.Driver"
}

for state in states_to_update:
    print(f"Processing state: {state}", flush=True)
    error_count = 0
    last_processed_logrecno = 0  # Reset for each state
    state_updates = 0

    while True:
        try:
            # Start timing the batch update
            start_time = time.time()

            # Fetch a batch of records using state and logrecno
            query = f"""
                SELECT sf1.state, sf1.logrecno, sf1.segment, sf1.tbl, dd.field_code
                FROM census.sf1_temp sf1
                JOIN (
                    SELECT 
                        segment, 
                        tbl, 
                        field_code,
                        ROW_NUMBER() OVER (PARTITION BY segment, tbl ORDER BY sort_id) - 1 AS index_position
                    FROM census.dd_seq_dhc
                    WHERE field_code IS NOT NULL
                ) dd
                ON sf1.segment::text = dd.segment::text
                AND UPPER(SPLIT_PART(sf1.tbl, '-', 1)) = UPPER(dd.tbl)
                AND CAST(SPLIT_PART(sf1.tbl, '-', 2) AS INTEGER) = dd.index_position
                WHERE sf1.state = '{state}' 
                AND sf1.logrecno > {last_processed_logrecno}
                AND sf1.tbl != dd.field_code  -- Only select rows where tbl needs to be updated
            """

            # Load the data into a Spark DataFrame
            df = spark.read.jdbc(url=jdbc_url, table=f"({query}) as batch_query", properties=connection_properties)

            if df.count() == 0:
                print(f"Completed processing for state {state}. Total updates: {state_updates}", flush=True)
                break

            # Repartition the DataFrame
            df = df.repartition(col("file_id"), col("state"))

            # Split the DataFrame into batches
            for partition in df.randomSplit([1.0 / batch_size] * batch_size):
                # Perform the update directly in Spark
                partition = partition.withColumn("new_tbl", expr("field_code"))

                # Write the updated rows back to the database
                partition.write \
                    .format("jdbc") \
                    .option("url", jdbc_url) \
                    .option("dbtable", "census.sf1_temp") \
                    .option("user", connection_properties["user"]) \
                    .option("password", connection_properties["password"]) \
                    .option("driver", connection_properties["driver"]) \
                    .mode("append") \
                    .save()

                # Update counters
                state_updates += partition.count()
                total_updates += partition.count()

            # Update the last processed logrecno
            last_processed_logrecno = df.agg({"logrecno": "max"}).collect()[0][0]

            # End timing the batch update
            end_time = time.time()
            elapsed_time = end_time - start_time

            print(f"Processed batch for state {state}. Current logrecno: {last_processed_logrecno}. "
                  f"Batch time: {elapsed_time:.2f} seconds", flush=True)

        except Exception as e:
            print(f"Error processing state {state}: {e}", flush=True)
            error_count += 1
            if error_count > 10:
                print(f"Too many errors in state {state}, skipping to next state.", flush=True)
                break

print(f"All updates completed. Total records updated: {total_updates}", flush=True)
spark.stop()
print("Spark session closed. Exiting program.", flush=True)
sys.exit(0)