from pyspark.sql import SparkSession
from pyspark.sql.functions import current_timestamp, lit, monotonically_increasing_id, from_json, col
from pyspark.sql.types import StructType, StructField, StringType, TimestampType
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def process_metadata(spark, source_tables, target_table):
    logger.info(f"Starting metadata processing. Target table: {target_table}")
    
    # Define the schema for the new table
    new_schema = StructType([
        StructField("id", StringType(), True),
        StructField("source_table", StringType(), True),
        StructField("file_name", StringType(), True),
        StructField("file_size", StringType(), True),
        StructField("file_path", StringType(), True),
        StructField("file_block_length", StringType(), True),
        StructField("file_type", StringType(), True),
        StructField("processed_time", TimestampType(), True)
    ])
    
    # Initialize an empty DataFrame with the new schema
    result_df = spark.createDataFrame([], new_schema)
    
    # Define the schema for the JSON data in source_metadata
    json_schema = StructType([
        StructField("file_name", StringType(), True),
        StructField("file_size", StringType(), True),
        StructField("file_path", StringType(), True),
        StructField("file_block_length", StringType(), True)
    ])
    
    for table in source_tables:
        logger.info(f"Processing table: {table}")
        
        try:
            # Read the source table
            source_df = spark.table(f"bk_mpo_raw.{table}")
            
            # Extract metadata from the JSON column
            metadata_df = source_df.select(
                lit(table).alias("source_table"),
                from_json(col("source_metadata"), json_schema).alias("json_data")
            ).select(
                "source_table",
                col("json_data.file_name").alias("file_name"),
                col("json_data.file_size").alias("file_size"),
                col("json_data.file_path").alias("file_path"),
                col("json_data.file_block_length").alias("file_block_length"),
                lit("").alias("file_type"),  # Assuming file_type is not in the JSON, adjust if needed
                current_timestamp().alias("processed_time")
            )
            
            # Get one unique record
            unique_metadata = metadata_df.dropDuplicates(["source_table", "file_name"]).limit(1)
            
            # Append to the result DataFrame
            result_df = result_df.union(unique_metadata)
            
            logger.info(f"Processed {table} successfully")
        except Exception as e:
            logger.error(f"Error processing table {table}: {str(e)}")
    
    # Add monotonically increasing ID
    result_df = result_df.withColumn("id", monotonically_increasing_id())
    
    # Persist the data to the specified target table
    try:
        result_df.write.mode("append").saveAsTable(target_table)
        logger.info(f"Data successfully persisted to {target_table}")
    except Exception as e:
        logger.error(f"Error persisting data to {target_table}: {str(e)}")
    
    logger.info("Metadata processing completed")
    return result_df

# Example usage
if __name__ == "__main__":
    spark = SparkSession.builder.appName("MetadataProcessor").getOrCreate()
    source_tables = ["LOAN_MONTH", "HELOC"]  # Add more tables as needed
    target_table = "test_bk_mpo.source_metadata_new"  # Specify your target table here
    result = process_metadata(spark, source_tables, target_table)
    result.show()
