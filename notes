# Import required libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import unix_timestamp, current_timestamp, lit, coalesce
from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType, BooleanType
import datetime

# Widget setup for parameters
dbutils.widgets.text("source_schema", "bk_mpo_raw_v1", "Source Schema")
dbutils.widgets.text("target_schema", "test_bk_mpo", "Target Schema")
dbutils.widgets.text("metadata_table_name", "test_bk_mpo.source_metadata_new", "Metadata Table Name")
dbutils.widgets.text("tables_to_process", "loan,loan_current,loan_delinquency_history,loan_lookup", "Tables to Process (comma-separated)")

# Read parameters
source_schema = dbutils.widgets.get("source_schema")
target_schema = dbutils.widgets.get("target_schema")
metadata_table_name = dbutils.widgets.get("metadata_table_name")
tables_input = dbutils.widgets.get("tables_to_process")
tables_to_process = [table.strip() for table in tables_input.split(",")]

def log_message(message):
    """
    Helper function to display logs in Databricks
    """
    log_df = spark.createDataFrame([(message,)], ["Log Message"])
    display(log_df)

class ETLProcessor:
    def __init__(self, spark, source_schema, target_schema, metadata_table_name):
        self.spark = spark
        self.source_schema = source_schema
        self.target_schema = target_schema
        self.metadata_table_name = metadata_table_name

    def truncate_and_insert_table_data(self, tables):
        """
        Process each table in the list
        """
        results = []
        for table_name in tables:
            try:
                # Construct full table names
                source_table = f"{self.source_schema}.{table_name}"
                target_table = f"{self.target_schema}.{table_name}_silver"
                
                log_message(f"Processing table: {table_name}")
                
                # Read the source table
                source_df = self.spark.table(source_table)
                
                # Extract metadata from the first row
                first_row_metadata = source_df.select("source_metadata.*").first()
                
                if first_row_metadata:
                    file_path = getattr(first_row_metadata, 'file_path', "unknown")
                    file_name = getattr(first_row_metadata, 'file_name', "unknown")
                    file_type = getattr(first_row_metadata, 'file_type', "")
                else:
                    log_message(f"Warning: No valid metadata found for table {table_name}")
                    file_path, file_name, file_type = "unknown", "unknown", ""
                
                # Insert all records into the target table with schema merging
                (source_df.write
                    .option("mergeSchema", "true")
                    .mode("overwrite")
                    .saveAsTable(target_table))
                
                rows_inserted = source_df.count()
                
                log_message(f"Insert operation completed for {table_name}")
                log_message(f"Source: {source_table}")
                log_message(f"Target: {target_table}")
                
                results.append((table_name, "PROCESSED", file_path, file_name, file_type, rows_inserted))
                
            except Exception as e:
                error_message = f"Error in insert operation for {table_name}: {str(e)}"
                log_message(error_message)
                results.append((table_name, "FAILED", "unknown", "unknown", "", 0))
        
        return results

    def update_source_metadata(self, file_path, project_name, file_name, file_process_state, file_type, rows_inserted, reprocess):
        """
        Update metadata with expanded schema
        """
        # Define the schema for our expanded metadata
        metadata_schema = StructType([
            StructField("id", LongType(), False),
            StructField("file_path", StringType(), True),
            StructField("project_name", StringType(), True),
            StructField("file_name", StringType(), True),
            StructField("process_date", TimestampType(), True),
            StructField("file_process_state", StringType(), True),
            StructField("file_type", StringType(), True),
            StructField("rows_inserted", LongType(), True),
            StructField("reprocess", BooleanType(), True)
        ])
        
        # Create a DataFrame with the metadata
        metadata_df = self.spark.createDataFrame([(
            0,  # Placeholder for id, will be replaced
            file_path or "unknown",
            project_name or "unknown",
            file_name or "unknown",
            None,  # Placeholder for process_date, will be replaced
            file_process_state or "unknown",
            file_type or "",
            rows_inserted,
            reprocess
        )], schema=metadata_schema)
        
        # Add UNIX_TIMESTAMP() for id and current_timestamp() for process_date
        metadata_df = metadata_df.withColumn("id", unix_timestamp().cast(LongType()))
        metadata_df = metadata_df.withColumn("process_date", current_timestamp())
        
        # Ensure no null values in non-nullable columns
        for field in metadata_schema.fields:
            if not field.nullable:
                metadata_df = metadata_df.withColumn(
                    field.name,
                    coalesce(metadata_df[field.name],
                            lit("unknown") if field.dataType == StringType()
                            else lit(False) if field.dataType == BooleanType()
                            else lit(0))
                )
        
        # Write the metadata to the table
        try:
            (metadata_df.write
                .mode("append")
                .option("mergeSchema", "true")
                .saveAsTable(self.metadata_table_name))
            
            log_message(f"Metadata inserted for project: {project_name}")
        except Exception as e:
            log_message(f"Error inserting metadata: {str(e)}")
            log_message("Attempted to insert the following data:")
            # metadata_df.show(truncate=False)

    def run_etl(self, tables_to_process):
        """
        Main ETL process
        """
        insert_results = self.truncate_and_insert_table_data(tables_to_process)
        overall_status = "PROCESSED" if all(status == "PROCESSED" for _, status, _, _, _, _ in insert_results) else "FAILED"
        
        # Process and update metadata for each table
        for table_name, status, file_path, file_name, file_type, rows_inserted in insert_results:
            project_name = f"{table_name}_bk_mpo_full_load"
            reprocess = status != "PROCESSED"
            
            self.update_source_metadata(
                file_path, project_name, file_name, status, 
                file_type, rows_inserted, reprocess
            )
        
        log_message(f"Overall status: {overall_status}")
        return insert_results, overall_status

# Main execution
try:
    log_message(f"""
    Starting ETL processing job with parameters:
    Source Schema: {source_schema}
    Target Schema: {target_schema}
    Metadata Table: {metadata_table_name}
    Tables to Process: {tables_to_process}
    """)

    # Initialize ETL processor
    etl_processor = ETLProcessor(
        spark,
        source_schema,
        target_schema,
        metadata_table_name
    )
    
    # Run ETL process
    results, overall_status = etl_processor.run_etl(tables_to_process)
    
    # Create summary DataFrame
    summary_schema = StructType([
        StructField("Table", StringType(), True),
        StructField("Status", StringType(), True),
        StructField("File Path", StringType(), True),
        StructField("File Name", StringType(), True),
        StructField("File Type", StringType(), True),
        StructField("Rows Inserted", LongType(), True)
    ])
    
    summary_df = spark.createDataFrame(results, schema=summary_schema)
    log_message("Processing Summary:")
    display(summary_df)
    
    # Return appropriate status
    if overall_status == "PROCESSED":
        dbutils.notebook.exit("SUCCESS")
    else:
        dbutils.notebook.exit("FAILED")
        
except Exception as e:
    log_message(f"Fatal error in ETL execution: {str(e)}")
    dbutils.notebook.exit("FAILED")
