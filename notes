from pyspark.sql import functions as F
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("UpdateEstCalendared").getOrCreate()

# Load your data into DataFrames
# Replace this with your actual data loading logic
exams_df = spark.table("rpt.exams")
event_df = spark.table("ses.event")

# Perform the join and update logic in Spark
result_df = exams_df.join(
    event_df.withColumnRenamed("namex", "event_namex"),
    exams_df.event_id == event_df.namex,
    "inner"
).withColumn(
    "est_calendared",
    F.when(
        (F.col("event_namex") == "Examination") & (exams_df.resource_level.isin("2", "3", "4")),
        F.lit("your_est_calendared_value")  # Replace with the logic to determine this value
    ).when(
        (F.col("event_namex") == "Follow-Up") & (exams_df.resource_level.isin("2", "3", "4")),
        F.lit("your_est_calendared_value")  # Replace with the logic to determine this value
    ).otherwise(F.col("est_calendared"))
)

# Assuming you want to overwrite the existing 'est_calendared' values in the rpt.exams table
# with the new DataFrame 'result_df'.
# This will not update the PostgreSQL table but will create/overwrite a Spark/Delta table.
result_df.write.mode("overwrite").format("delta").saveAsTable("rpt.exams")

# If you need to update the PostgreSQL table, you would use JDBC to write the updates
# which is more complex and typically involves writing to a temporary table, then using a SQL
# "UPDATE FROM" statement to update the original PostgreSQL table from that temporary table.

spark.stop()
