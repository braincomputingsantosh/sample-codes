from pyspark.sql import SparkSession
from pyspark.sql.functions import current_timestamp, lit, col, monotonically_increasing_id, substring_index, max, regexp_extract
from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType
from delta.tables import DeltaTable
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def process_metadata(spark, source_tables, target_table, default_year, default_month):
    logger.info(f"Starting metadata processing. Target table: {target_table}")
    
    # Define the schema for the new table
    new_schema = StructType([
        StructField("id", LongType(), False),
        StructField("project_name", StringType(), False),
        StructField("source_table", StringType(), True),
        StructField("file_name", StringType(), True),
        StructField("file_path", StringType(), True),
        StructField("file_extension", StringType(), True),
        StructField("file_block_length", StringType(), True),
        StructField("file_type", StringType(), True),
        StructField("processed_time", TimestampType(), True),
        StructField("year", StringType(), True),
        StructField("month", StringType(), True)
    ])
    
    # Get the maximum ID from the target table
    try:
        max_id = spark.table(target_table).agg(max("id")).collect()[0][0]
        id_offset = max_id + 1 if max_id is not None else 1000000
    except Exception as e:
        logger.warning(f"Could not retrieve max ID from target table: {str(e)}. Using default offset.")
        id_offset = 1000000
    
    for table in source_tables:
        logger.info(f"Processing table: {table}")
        
        try:
            # Read the source table
            source_df = spark.table(f"staging.bk_mpo_raw.{table}")
            
            # Extract metadata from the STRUCT column
            metadata_df = source_df.select(
                (monotonically_increasing_id() + id_offset).alias("id"),
                lit("bk_mpo").alias("project_name"),
                lit(table).cast(StringType()).alias("source_table"),
                col("source_metadata.file_name").cast(StringType()).alias("file_name"),
                col("source_metadata.file_path").cast(StringType()).alias("file_path"),
                substring_index(col("source_metadata.file_path"), ".", -1).alias("file_extension"),
                col("source_metadata.file_block_length").cast(StringType()).alias("file_block_length"),
                lit("").cast(StringType()).alias("file_type"),
                current_timestamp().alias("processed_time")
            )
            
            # Extract year and month from file path for LOAN_MONTH table
            if table == "LOAN_MONTH":
                metadata_df = metadata_df.withColumn("year", regexp_extract(col("file_path"), r"/(\d{4})/", 1))
                metadata_df = metadata_df.withColumn("month", regexp_extract(col("file_path"), r"LoanMonth_(\d)\d{2}", 1))
            else:
                metadata_df = metadata_df.withColumn("year", lit(default_year))
                metadata_df = metadata_df.withColumn("month", lit(default_month))
            
            # Get one unique record
            unique_metadata = metadata_df.limit(1)
            
            # Perform upsert operation
            try:
                # Create a DeltaTable object
                delta_table = DeltaTable.forName(spark, target_table)
                
                # Perform the merge operation
                delta_table.alias("target").merge(
                    unique_metadata.alias("updates"),
                    "target.source_table = updates.source_table AND target.file_path = updates.file_path"
                ).whenMatchedUpdate(set={
                    "file_name": "updates.file_name",
                    "file_extension": "updates.file_extension",
                    "file_block_length": "updates.file_block_length",
                    "file_type": "updates.file_type",
                    "processed_time": "updates.processed_time",
                    "year": "updates.year",
                    "month": "updates.month"
                }).whenNotMatchedInsert(values={
                    "id": "updates.id",
                    "project_name": "updates.project_name",
                    "source_table": "updates.source_table",
                    "file_name": "updates.file_name",
                    "file_path": "updates.file_path",
                    "file_extension": "updates.file_extension",
                    "file_block_length": "updates.file_block_length",
                    "file_type": "updates.file_type",
                    "processed_time": "updates.processed_time",
                    "year": "updates.year",
                    "month": "updates.month"
                }).execute()
                
                logger.info(f"Data successfully upserted to {target_table} for {table}")
            except Exception as e:
                logger.error(f"Error upserting data to {target_table} for {table}: {str(e)}")
            
        except Exception as e:
            logger.error(f"Error processing table {table}: {str(e)}")
    
    logger.info("Metadata processing completed")
    return spark.table(target_table)

# Example usage
if __name__ == "__main__":
    spark = SparkSession.builder.appName("MetadataProcessor").getOrCreate()
    source_tables = ["LOAN_MONTH", "HELOC", "LOAN_MONTH_LOSS_MIT"]
    target_table = "test_bk_mpo.source_metadata_new"
    default_year = "2024"
    default_month = "04"  # Default month for tables other than LOAN_MONTH
    result = process_metadata(spark, source_tables, target_table, default_year, default_month)
    result.show()
