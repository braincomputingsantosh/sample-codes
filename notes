from pyspark.sql import SparkSession

# Create Spark session
spark = SparkSession.builder \
    .appName("PostgreSQL Integration") \
    .getOrCreate()

# PostgreSQL database details
database_url = "jdbc:postgresql://[host]:[port]/[database]"
properties = {
    "user": "[username]",
    "password": "[password]",
    "driver": "org.postgresql.Driver"
}

# Load data from a PostgreSQL table
df = spark.read.jdbc(url=database_url, table="[table_name]", properties=properties)

# Show some data
df.show()

# Optionally, you can also write data back to the database
# df.write.jdbc(url=database_url, table="[table_name]", mode="append", properties=properties)

# Stop the Spark session
spark.stop()
