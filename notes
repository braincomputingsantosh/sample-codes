from pyspark.sql import SparkSession
from pyspark.sql.functions import current_timestamp, lit, from_json, col
from pyspark.sql.types import StructType, StructField, StringType, TimestampType
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def process_metadata(spark, source_tables, target_table, source_year, source_month):
    # Ensure source_month is a two-digit string
    source_month = f"{int(source_month):02d}"
    
    logger.info(f"Starting metadata processing. Target table: {target_table}, Year: {source_year}, Month: {source_month}")
    
    # Define the schema for the new table
    new_schema = StructType([
        StructField("source_table", StringType(), True),
        StructField("file_name", StringType(), True),
        StructField("file_size", StringType(), True),
        StructField("file_path", StringType(), True),
        StructField("file_block_length", StringType(), True),
        StructField("file_type", StringType(), True),
        StructField("processed_time", TimestampType(), True)
    ])
    
    # Initialize an empty DataFrame with the new schema
    result_df = spark.createDataFrame([], new_schema)
    
    # Define the schema for the JSON data in source_metadata
    json_schema = StructType([
        StructField("file_name", StringType(), True),
        StructField("file_size", StringType(), True),
        StructField("file_path", StringType(), True),
        StructField("file_block_length", StringType(), True)
    ])
    
    for table in source_tables:
        logger.info(f"Processing table: {table}")
        
        try:
            # Read the source table
            source_df = spark.table(f"staging.bk_mpo_raw.{table}")
            
            # Simplified file path pattern for HELOC and LOAN_MONTH_LOSS_MIT
            file_path_pattern = f"/{source_year}/{source_month}"
            
            filtered_df = source_df.filter(col("source_metadata.file_path").contains(file_path_pattern))
            
            # If no matching records, log and continue to next table
            if filtered_df.count() == 0:
                logger.warning(f"No matching records found for {table} in {source_year}/{source_month}")
                continue
            
            # Extract metadata from the JSON column
            metadata_df = filtered_df.select(
                lit(table).alias("source_table"),
                from_json(col("source_metadata"), json_schema).alias("json_data")
            ).select(
                "source_table",
                col("json_data.file_name").alias("file_name"),
                col("json_data.file_size").cast(StringType()).alias("file_size"),
                col("json_data.file_path").alias("file_path"),
                col("json_data.file_block_length").cast(StringType()).alias("file_block_length"),
                lit("").alias("file_type"),
                current_timestamp().alias("processed_time")
            )
            
            # Get one unique record
            unique_metadata = metadata_df.limit(1)
            
            # Append to the result DataFrame
            result_df = result_df.union(unique_metadata)
            
            logger.info(f"Processed {table} successfully for {source_year}/{source_month}")
        except Exception as e:
            logger.error(f"Error processing table {table}: {str(e)}")
    
    # Persist the data to the specified target table
    try:
        result_df.write.mode("append").saveAsTable(target_table)
        logger.info(f"Data successfully persisted to {target_table}")
    except Exception as e:
        logger.error(f"Error persisting data to {target_table}: {str(e)}")
    
    logger.info("Metadata processing completed")
    return result_df

# Example usage
if __name__ == "__main__":
    spark = SparkSession.builder.appName("MetadataProcessor").getOrCreate()
    source_tables = ["LOAN_MONTH", "HELOC", "LOAN_MONTH_LOSS_MIT"]
    target_table = "test_bk_mpo.source_metadata_new"
    source_year = "2024"
    source_month = "04"  # April
    result = process_metadata(spark, source_tables, target_table, source_year, source_month)
    result.show()
