"""
ETL Process for BK MPO Data Pipeline

This script implements an ETL (Extract, Transform, Load) process for handling data from multiple tables
in a staging schema and loading it into a processed schema. It also records metadata about each operation.

Key Features:
1. Processes multiple tables: loan, loan_current, loan_delinquency_history, and loan_lookup.
2. Extracts data from a source schema (staging.bk_mpo_raw) and loads it into a target schema (staging.bk_mpo_processed).
3. Uses schema merging to handle potential schema evolution between source and target tables.
4. Captures and stores metadata for each processed table, including:
   - Unique ID (based on UNIX timestamp)
   - File path, name, and type
   - Project name
   - Processing date and state
   - Number of rows inserted
   - Reprocess flag (set to True for failed operations, False for successful ones)
5. Implements error handling and logging throughout the ETL process.
6. Designed to run in a serverless Spark environment.

Usage:
- Ensure that the necessary Spark environment and dependencies are set up.
- The main ETL job is initiated by the run_etl_job function, which creates an ETLProcessor instance
  and processes the specified tables.
- Metadata for each operation is stored in the test_bk_mpo.source_metadata_new table.

Note: This script is designed to be run as a standalone Spark job. Ensure that the source and target
schemas, as well as the metadata table, exist and are accessible in your Spark environment.
"""
