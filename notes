import datetime
import os
from pyspark.sql.functions import col, split, element_at
from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType

class ETLProcessor:
    def __init__(self, spark, source_schema, target_schema, metadata_table_name):
        self.spark = spark
        self.source_schema = source_schema
        self.target_schema = target_schema
        self.metadata_table_name = metadata_table_name

    def truncate_and_insert_table_data(self, tables):
        results = []
        for table_name in tables:
            try:
                source_table = f"{self.source_schema}.{table_name}"
                target_table = f"{self.target_schema}.{table_name}"

                # Read the source table
                source_df = self.spark.table(source_table)

                # Print schema and sample data for debugging
                print(f"Schema for table {table_name}:")
                source_df.printSchema()
                print(f"Sample data for table {table_name}:")
                source_df.select("source_metadata").show(1, truncate=False)

                # Extract metadata from the first row
                first_row_metadata = source_df.select("source_metadata").first()

                if first_row_metadata and first_row_metadata.source_metadata:
                    metadata_parts = first_row_metadata.source_metadata.split(", ")
                    file_path = metadata_parts[0]
                    file_name = file_path.split("/")[-1]
                    file_size = int(metadata_parts[-2]) if len(metadata_parts) > 2 else 0
                else:
                    print(f"Warning: No valid metadata found for table {table_name}")
                    file_path = "unknown"
                    file_name = "unknown"
                    file_size = 0

                # Insert all records into the target table with schema merging
                (source_df.write
                 .option("mergeSchema", "true")
                 .option("overwriteSchema", "true")
                 .mode("overwrite")
                 .saveAsTable(target_table))

                rows_inserted = source_df.count()

                print(f"Truncate and insert operation completed for {table_name}")
                print(f"Source: {source_table}")
                print(f"Target: {target_table}")
                results.append((table_name, "SUCCESS", file_path, file_name, file_size, rows_inserted))
            except Exception as e:
                error_message = f"Error in truncate and insert operation for {table_name}: {str(e)}"
                print(error_message)
                results.append((table_name, "FAILED", None, None, None, 0))

        return results

    # ... (rest of the methods remain the same)

# Usage example for serverless environment
def run_etl_job(spark):
    etl_processor = ETLProcessor(spark, 
                                 source_schema="staging.bk_mpo_raw", 
                                 target_schema="staging.bk_mpo_processed", 
                                 metadata_table_name="test_bk_mpo.source_metadata_new")

    tables_to_process = ["loan", "loan_current", "loan_delinquency_history", "loan_lookup"]
    results, overall_status = etl_processor.run_etl(tables_to_process)

    print(f"ETL Process completed with status: {overall_status}")
    for table, status, file_path, file_name, file_size, rows_inserted in results:
        print(f"Table: {table}, Status: {status}, File: {file_name}, Rows Inserted: {rows_inserted}")

# This is the entry point for the serverless job
if __name__ == "__main__":
    from pyspark.sql import SparkSession
    spark = SparkSession.builder.appName("ETLProcessor").getOrCreate()
    
    run_etl_job(spark)
