from pyspark.sql.functions import to_timestamp
from pyspark.sql.types import StringType, TimestampType

def update_source_metadata(spark, metadata_table_name, metadata):
    # Ensure process_date is in the correct format
    if isinstance(metadata['process_date'], str):
        try:
            # If it's a string, try to parse it as a timestamp
            metadata['process_date'] = to_timestamp(metadata['process_date']).cast(TimestampType())
        except:
            # If parsing fails, use current timestamp
            metadata['process_date'] = to_timestamp(current_timestamp())
    elif not isinstance(metadata['process_date'], TimestampType):
        # If it's neither a string nor a timestamp, use current timestamp
        metadata['process_date'] = to_timestamp(current_timestamp())

    # Create a DataFrame from the metadata
    metadata_df = spark.createDataFrame([metadata])

    # Get the schema of the target table
    target_schema = spark.table(metadata_table_name).schema

    # Cast the columns of metadata_df to match the target schema
    for field in target_schema.fields:
        if field.name in metadata_df.columns:
            metadata_df = metadata_df.withColumn(field.name, metadata_df[field.name].cast(field.dataType))

    # Perform the merge operation
    spark.sql(f"""
    MERGE INTO {metadata_table_name} AS target
    USING (SELECT * FROM metadata_df) AS source
    ON target.file_path = source.file_path
    WHEN MATCHED THEN
        UPDATE SET *
    WHEN NOT MATCHED THEN
        INSERT *
    """)

    print(f"Metadata updated for file: {metadata['file_name']}")

# In the append_and_update_metadata function, update the metadata creation:
metadata = {
    "project_name": "bk_mpo",
    "file_name": file_info["file_name"],
    "file_path": file_path,
    "process_date": datetime.now(),  # Use datetime object directly
    "file_type": file_info["file_type"],
    "file_process_state": process_state,
    "rows_inserted": rows_inserted,
    "last_error_msg": "",
    "reprocess": "N"
}
