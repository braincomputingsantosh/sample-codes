from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from delta.tables import *

# Create a Spark session
spark = SparkSession.builder.appName("CorrectStreamingUpsert").getOrCreate()

# Create a sample Silver table if it doesn't exist
spark.sql("""
CREATE TABLE IF NOT EXISTS silver_customer_table (
  customer_id INT,
  name STRING,
  email STRING,
  updated_at TIMESTAMP
) USING DELTA
""")

# Insert some initial data
spark.sql("""
INSERT INTO silver_customer_table VALUES 
  (1, 'John Doe', 'john.doe@example.com', current_timestamp()),
  (2, 'Jane Smith', 'jane.smith@example.com', current_timestamp())
""")

# Define the schema for our streaming data
schema = StructType([
    StructField("customer_id", IntegerType(), True),
    StructField("name", StringType(), True),
    StructField("email", StringType(), True),
    StructField("updated_at", TimestampType(), True)
])

# Create a streaming source using rate source and add mock data
stream_df = (spark.readStream.format("rate").option("rowsPerSecond", 1).load()
             .withColumn("customer_id", (col("value") % 4 + 1).cast(IntegerType()))
             .withColumn("name", when(col("customer_id") == 1, "John Doe")
                         .when(col("customer_id") == 2, "Jane Smith")
                         .when(col("customer_id") == 3, "Alice Johnson")
                         .otherwise("Bob Wilson"))
             .withColumn("email", concat(lower(col("name").substr(1, 1)), 
                                         lower(split(col("name"), " ")[1]), 
                                         lit("@example.com")))
             .withColumn("updated_at", current_timestamp())
             .select("customer_id", "name", "email", "updated_at"))

# Define the upsert function
def upsert_to_silver(microBatchOutputDF, batchId):
    # Create a Delta table object for the target table
    delta_table = DeltaTable.forName(spark, "silver_customer_table")

    # Perform the upsert operation
    (delta_table.alias("target")
     .merge(microBatchOutputDF.alias("source"), "target.customer_id = source.customer_id")
     .whenMatchedUpdateAll()
     .whenNotMatchedInsertAll()
     .execute())

    print(f"Batch {batchId} completed")

# Start the streaming query
query = (stream_df.writeStream
         .foreachBatch(upsert_to_silver)
         .outputMode("update")
         .trigger(processingTime='10 seconds')
         .start())

# Wait for the streaming job to process a few batches
import time
time.sleep(60)  # Run for 60 seconds

# Stop the query
query.stop()

# Show the final results
print("Silver table after streaming upserts:")
spark.sql("SELECT * FROM silver_customer_table ORDER BY customer_id").show()

# Clean up (uncomment if you want to drop the table after the demo)
# spark.sql("DROP TABLE IF EXISTS silver_customer_table")
