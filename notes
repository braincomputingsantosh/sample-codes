def update_source_metadata(self, metadata):
    update_sql = f"""
    INSERT INTO {self.metadata_table_name} 
    (project_name, file_name, file_path, process_date, file_type, file_process_state, rows_inserted, file_size, last_error_msg, reprocess)
    VALUES (
        '{metadata.get("project_name", "")}',
        '{metadata.get("file_name", "")}',
        '{metadata.get("file_path", "")}',
        '{metadata.get("process_date", "")}',
        '{metadata.get("file_type", "")}',
        '{metadata.get("file_process_state", "")}',
        {metadata.get("rows_inserted") if metadata.get("rows_inserted") is not None else "NULL"},
        {metadata.get("file_size") if metadata.get("file_size") is not None else "NULL"},
        '{metadata.get("last_error_msg", "")}',
        {str(metadata.get("reprocess", False)).lower()}
    )
    """
    try:
        self.spark.sql(update_sql)
        print(f"Metadata inserted for file: {metadata.get('file_name')}")
    except Exception as e:
        print(f"Error inserting metadata: {str(e)}")
        print(f"Attempted SQL: {update_sql}")
