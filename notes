from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from delta.tables import *
import time

# Create a Spark session
spark = SparkSession.builder.appName("BK_MPO_UpsertStreaming").getOrCreate()

source_table = 'bk_mpo_raw.loan_arm'
target_table = 'test_bk_mpo.test_loan_arm'
merge_condition = "target.loan_id = source.loan_id"

# Define the upsert function
def upsert_to_silver(microBatchOutputDF, batchId):
    try:
        # Create a Delta table object for the target table
        delta_table = DeltaTable.forName(spark, target_table)
        
        # Perform the upsert operation
        (delta_table.alias("target")
         .merge(microBatchOutputDF.alias("source"), merge_condition)
         .whenMatchedUpdateAll()
         .whenNotMatchedInsertAll()
         .execute())
        
        print(f"Batch {batchId} completed")
    except Exception as e:
        print(f"Error in batch {batchId}: {str(e)}")
        raise  # Re-raise the exception to stop the stream

# Function to get and print table schema
def print_table_schema(table_name):
    try:
        schema = spark.table(table_name).schema
        print(f"Schema of {table_name}:")
        for field in schema.fields:
            print(f"- {field.name}: {field.dataType}")
    except Exception as e:
        print(f"Error reading schema of {table_name}: {str(e)}")

# Print schemas of both tables
print_table_schema(source_table)
print_table_schema(target_table)

try:
    # Read from the source table as a stream
    source_stream = (spark.readStream
                     .format("delta")
                     .table(source_table)
                     .withColumn("processed_timestamp", current_timestamp()))

    # Start the streaming query
    query = (source_stream.writeStream
             .foreachBatch(upsert_to_silver)
             .outputMode("update")
             .trigger(processingTime='100 seconds')
             .start())

    # Monitor the streaming query
    while query.isActive:
        print(f"Query status: {query.status}")
        if query.lastProgress:
            print(f"Last progress: {query.lastProgress}")
        time.sleep(10)  # Wait for 10 seconds before checking again

except Exception as e:
    print(f"Error starting or running the streaming query: {str(e)}")
finally:
    if 'query' in locals() and query.isActive:
        query.stop()
    print("Streaming query has stopped.")

# After stopping, you can check the target table:
print("Final state of the target table:")
spark.sql(f"SELECT COUNT(*) FROM {target_table}").show()
spark.sql(f"SELECT * FROM {target_table} LIMIT 5").show()
