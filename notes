https://www.census.gov/programs-surveys/acs/data/summary-file/sequence-based/2010.html

https://www2.census.gov/programs-surveys/decennial/2010/technical-documentation/complete-tech-docs/summary-file/dpsf.pdf

https://www.census.gov/data/datasets/2010/dec/summary-file-1.html

WITH indexed_dd AS (
  SELECT 
    segment, 
    tbl, 
    field_code,
    ROW_NUMBER() OVER (PARTITION BY segment, tbl ORDER BY sort_id) - 1 AS index_position
  FROM census.dd_seq_dhc
  WHERE field_code IS NOT NULL
)
UPDATE census.sf1_temp sf1
SET tbl = dd.field_code
FROM indexed_dd dd
WHERE sf1.segment::text = dd.segment::text
  AND UPPER(SPLIT_PART(sf1.tbl, '-', 1)) = UPPER(dd.tbl)
  AND CAST(SPLIT_PART(sf1.tbl, '-', 2) AS INTEGER) = dd.index_position;


Here's an explanation of the updated code:

This SQL query is designed to update our census data table (sf1_temp) with more detailed information from another table (dd_seq_dhc). Here's what it does:

1. First, we prepare our reference data (dd_seq_dhc). We select the important columns (segment, tbl, and field_code) and create a new column called index_position. This index_position is crucial because it helps us match rows between our two tables correctly.

2. In our main census table (sf1_temp), we have entries like "p1-0" or "p1-1" in the tbl column. The number after the hyphen is important - it tells us which specific piece of information we're dealing with.

3. We then update sf1_temp by matching each row with the correct row from our prepared dd_seq_dhc data. We match based on three things:
   - The segment values must be the same
   - The first part of sf1_temp's tbl (before the hyphen) must match dd_seq_dhc's tbl
   - The number after the hyphen in sf1_temp's tbl must match our calculated index_position

4. When we find a match, we replace the entire tbl value in sf1_temp with the field_code from dd_seq_dhc. This means we're swapping out codes like "p1-0" for more descriptive identifiers.


Certainly. I'll provide a more detailed and explanatory breakdown of the CTE approach, which should help you explain it to a data steward. This explanation will focus on the concept, purpose, and benefits of using a CTE in this context.

Explaining the CTE Approach to a Data Steward:

1. Introduction to CTEs:
   "A Common Table Expression, or CTE, is like creating a temporary named result set that we can reference within our main query. It's a way to organize complex queries into more manageable, readable pieces."

2. Purpose of the CTE in this scenario:
   "In our case, we're using a CTE to prepare and structure the data from the census.dd_seq_dhc table before we use it in our main update operation. This preparation step helps us handle the complexity of matching index positions and ensures our update logic is clear and efficient."

3. Breaking down the CTE (indexed_dd):
   "Let's look at what our CTE, named 'indexed_dd', does:
   a) It selects specific columns we need: segment, tbl, and field_code.
   b) It creates a new column called index_position. This is crucial because it assigns a number (starting from 0) to each row within groups defined by segment and tbl.
   c) It only includes rows where field_code is not null, as we're not interested in empty values."

4. Explaining the ROW_NUMBER() function:
   "We use a special function called ROW_NUMBER() to create our index_position. It's like assigning race numbers to runners, but we're grouping by segment and tbl, and ordering by sort_id. We subtract 1 from this number so our index starts at 0 instead of 1."

5. Benefits of using the CTE:
   "By preparing our data this way, we achieve several benefits:
   a) Improved readability: The main query becomes easier to understand because the complex logic of creating index positions is separated.
   b) Reusability: If we needed to use this indexed data multiple times in our query, we could easily reference the CTE without repeating the logic.
   c) Optimization: The database can sometimes optimize CTEs better than nested subqueries."

6. The Main Update Query:
   "After preparing our data with the CTE, our main update query becomes more straightforward:
   a) We join our census.sf1_temp table with our prepared indexed_dd data.
   b) We match rows based on segment, the first part of the tbl column, and the index position.
   c) When we find a match, we update the tbl column by adding the corresponding field_code."

7. Handling Data Inconsistencies:
   "Our approach also handles potential data inconsistencies:
   a) We use UPPER() to ignore case differences when matching tbl values.
   b) We cast the segment to text to ensure we can compare it across tables, even if the data types are different.
   c) We use SPLIT_PART() to separate the tbl value into its base and index components, allowing for flexible matching."

Conclusion:
"In essence, this CTE approach allows us to perform a complex update operation in a structured, readable, and efficient manner. It prepares our reference data (from dd_seq_dhc) in a format that makes it easy to match and update our main data (in sf1_temp), handling various data formats and potential inconsistencies along the way."

This explanation provides a comprehensive overview of the CTE approach, its benefits, and how it addresses the specific requirements of your data update task. It should give a data steward a clear understanding of the process and its advantages.

5. We use UPPER() and text conversion to make sure we don't miss matches due to differences in letter case or data types.

This approach ensures that each entry in our main census table gets accurately paired with its corresponding detailed information, replacing general codes with more specific identifiers. It's a careful way of enriching our census data while maintaining its integrity and accuracy.


*********** GEOID *******************

filePath = '/data/2024_sf1_geo/dcgeo2010.sf1'

column_specs = [
    {"name": "FILEID", "start": 1, "length": 6},
    {"name": "STUSAB", "start": 7, "length": 2},
    {"name": "SUMLEVEL", "start": 9, "length": 3},
    {"name": "GEOCOMP", "start": 12, "length": 2},
    {"name": "CHARITER", "start": 14, "length": 3},
    {"name": "CIFSN", "start": 17, "length": 2},
    {"name": "LOGRECNO", "start": 19, "length": 7},
    {"name": "REGION", "start": 26, "length": 1},
    {"name": "DIVISION", "start": 27, "length": 1},
    {"name": "STATE", "start": 28, "length": 2},
    {"name": "COUNTY", "start": 30, "length": 3},
    {"name": "TRACT", "start": 55, "length": 6},
    {"name": "COUNTYCC", "start": 33, "length": 2},
    {"name": "COUNTYSC", "start": 35, "length": 2},
    {"name": "COUSUB", "start": 37, "length": 5},
    {"name": "COUSUBCC", "start": 42, "length": 2},
    {"name": "COUSUBSC", "start": 44, "length": 2},
    {"name": "PLACE", "start": 46, "length": 5},
    {"name": "PLACECC", "start": 51, "length": 2},
    {"name": "PLACESC", "start": 53, "length": 2},
    {"name": "BLKGRP", "start": 61, "length": 1},
    {"name": "BLOCK", "start": 62, "length": 4}
]

def parsed_line(line):
    parsed_data = {}
    for field in column_specs:
        startPos = field["start"] - 1
        endPos = startPos + field["length"]
        parsed_data[field["name"]] = line[startPos:endPos].strip()
    return parsed_data

parsed_records = []
with open(filePath, 'r', encoding='latin1') as file:
    for line in file:
        parsed_record = parsed_line(line)
        # Create GEOID by combining STATE, COUNTY, TRACT, and BLOCK
        parsed_record['GEOID'] = (
            parsed_record['STATE'] +
            parsed_record['COUNTY'] +
            parsed_record['TRACT'] +
            parsed_record['BLOCK']
        )
        parsed_records.append(parsed_record)

import pandas as pd
df = pd.DataFrame(parsed_records)

# Add GEOID to column_specs for the CREATE TABLE statement
column_specs.append({"name": "GEOID", "start": 0, "length": 0})  # Dummy values for start and length

columnDefinitions = ', '.join([f'{field["name"]} TEXT' for field in column_specs])
sqlCreateTableQuery = f'CREATE TABLE IF NOT EXISTS sf1_data_2010 ({columnDefinitions})'
print(sqlCreateTableQuery)

import sqlite3

conn = sqlite3.connect('sf1_dc_geo_data.db')
cursor = conn.cursor()

# Create the table
cursor.execute(sqlCreateTableQuery)

# Insert the data
df.to_sql('sf1_data_2010', conn, if_exists='replace', index=False)

conn.close()

print("Data has been successfully loaded into the SQLite database.")


******** Validate GEOID **************

import pandas as pd
import geopandas as gpd
import folium

# Load your data
df = pd.read_sql_query("SELECT * FROM sf1_data_2010", conn)

# If you have a shapefile with GEOID and geometry, use this:
# gdf = gpd.read_file('path_to_your_shapefile.shp')
# df = df.merge(gdf[['GEOID', 'geometry']], on='GEOID', how='left')

# For this example, let's assume you have lat/lon columns
# If not, you'll need to geocode your GEOIDs or join with a shapefile

# Create a map centered on the mean lat/lon
center_lat = df['LATITUDE'].mean()
center_lon = df['LONGITUDE'].mean()
m = folium.Map(location=[center_lat, center_lon], zoom_start=10)

# Add markers for each GEOID
for idx, row in df.iterrows():
    folium.Marker(
        location=[row['LATITUDE'], row['LONGITUDE']],
        popup=f"GEOID: {row['GEOID']}<br>STATE: {row['STATE']}<br>COUNTY: {row['COUNTY']}<br>TRACT: {row['TRACT']}<br>BLOCK: {row['BLOCK']}",
        tooltip=row['GEOID']
    ).add_to(m)

# Save the map
m.save('geoid_validation_map.html')

print("Map has been saved as 'geoid_validation_map.html'")


******* New Code **************

import sqlite3
import pandas as pd
import folium

# Connect to the SQLite database
conn = sqlite3.connect('sf1_dc_geo_data.db')

# Query to select data from the database
# You may need to adjust this query based on your actual table structure
query = """
SELECT GEOID, STATE, COUNTY, TRACT, BLOCK, 
       CAST(SUBSTR(TRACT, 1, 2) || '.' || SUBSTR(TRACT, 3) AS FLOAT) AS LAT, 
       CAST(SUBSTR(BLOCK, 1, 2) || '.' || SUBSTR(BLOCK, 3) AS FLOAT) AS LON
FROM sf1_data_2010
LIMIT 1000  -- Limiting to 1000 records for performance, adjust as needed
"""

# Execute the query and load results into a DataFrame
df = pd.read_sql_query(query, conn)

# Close the database connection
conn.close()

# Create a map centered on the mean lat/lon
center_lat = df['LAT'].mean()
center_lon = df['LON'].mean()
m = folium.Map(location=[center_lat, center_lon], zoom_start=10)

# Add markers for each GEOID
for idx, row in df.iterrows():
    folium.Marker(
        location=[row['LAT'], row['LON']],
        popup=f"GEOID: {row['GEOID']}<br>STATE: {row['STATE']}<br>COUNTY: {row['COUNTY']}<br>TRACT: {row['TRACT']}<br>BLOCK: {row['BLOCK']}",
        tooltip=row['GEOID']
    ).add_to(m)

# Save the map
m.save('geoid_validation_map.html')

print("Map has been saved as 'geoid_validation_map.html'")

