from pyspark.sql import SparkSession
from pyspark.sql.functions import current_timestamp, lit, col, monotonically_increasing_id, substring_index, max
from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType
from delta.tables import DeltaTable
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def process_metadata(spark, source_tables, target_table, source_year, source_month):
    # Ensure source_month is a two-digit string
    source_month = f"{int(source_month):02d}"
    
    logger.info(f"Starting metadata processing. Target table: {target_table}, Year: {source_year}, Month: {source_month}")
    
    # Define the schema for the new table
    new_schema = StructType([
        StructField("id", LongType(), False),
        StructField("project_name", StringType(), False),
        StructField("source_table", StringType(), True),
        StructField("file_name", StringType(), True),
        StructField("file_path", StringType(), True),
        StructField("file_extension", StringType(), True),
        StructField("file_block_length", StringType(), True),
        StructField("file_type", StringType(), True),
        StructField("processed_time", TimestampType(), True)
    ])
    
    # Initialize an empty DataFrame with the new schema
    result_df = spark.createDataFrame([], new_schema)
    
    # Get the maximum ID from the target table
    try:
        max_id = spark.table(target_table).agg(max("id")).collect()[0][0]
        id_offset = max_id + 1 if max_id is not None else 1000000
    except Exception as e:
        logger.warning(f"Could not retrieve max ID from target table: {str(e)}. Using default offset.")
        id_offset = 1000000
    
    for table in source_tables:
        logger.info(f"Processing table: {table}")
        
        try:
            # Read the source table
            source_df = spark.table(f"staging.bk_mpo_raw.{table}")
            
            # Filter for the specific year/month
            file_path_pattern = f"/{source_year}/{source_month}"
            
            filtered_df = source_df.filter(col("source_metadata.file_path").contains(file_path_pattern))
            
            # If no matching records, log and continue to next table
            if filtered_df.count() == 0:
                logger.warning(f"No matching records found for {table} in {source_year}/{source_month}")
                continue
            
            # Extract metadata from the STRUCT column
            metadata_df = filtered_df.select(
                (monotonically_increasing_id() + id_offset).alias("id"),
                lit("bk_mpo").alias("project_name"),
                lit(table).cast(StringType()).alias("source_table"),
                col("source_metadata.file_name").cast(StringType()).alias("file_name"),
                col("source_metadata.file_path").cast(StringType()).alias("file_path"),
                substring_index(col("source_metadata.file_path"), ".", -1).alias("file_extension"),
                col("source_metadata.file_block_length").cast(StringType()).alias("file_block_length"),
                lit("").cast(StringType()).alias("file_type"),
                current_timestamp().alias("processed_time")
            )
            
            # Get one unique record
            unique_metadata = metadata_df.limit(1)
            
            # Append to the result DataFrame
            result_df = result_df.union(unique_metadata)
            
            logger.info(f"Processed {table} successfully for {source_year}/{source_month}")
        except Exception as e:
            logger.error(f"Error processing table {table}: {str(e)}")
    
    # Perform upsert operation
    try:
        # Create a DeltaTable object
        delta_table = DeltaTable.forName(spark, target_table)
        
        # Perform the merge operation
        delta_table.alias("target").merge(
            result_df.alias("updates"),
            "target.source_table = updates.source_table AND target.file_path = updates.file_path"
        ).whenMatchedUpdate(set={
            "file_name": "updates.file_name",
            "file_extension": "updates.file_extension",
            "file_block_length": "updates.file_block_length",
            "file_type": "updates.file_type",
            "processed_time": "updates.processed_time"
        }).whenNotMatchedInsert(values={
            "id": "updates.id",
            "project_name": "updates.project_name",
            "source_table": "updates.source_table",
            "file_name": "updates.file_name",
            "file_path": "updates.file_path",
            "file_extension": "updates.file_extension",
            "file_block_length": "updates.file_block_length",
            "file_type": "updates.file_type",
            "processed_time": "updates.processed_time"
        }).execute()
        
        logger.info(f"Data successfully upserted to {target_table}")
    except Exception as e:
        logger.error(f"Error upserting data to {target_table}: {str(e)}")
    
    logger.info("Metadata processing completed")
    return result_df

# Example usage
if __name__ == "__main__":
    spark = SparkSession.builder.appName("MetadataProcessor").getOrCreate()
    source_tables = ["LOAN_MONTH", "HELOC", "LOAN_MONTH_LOSS_MIT"]
    target_table = "test_bk_mpo.source_metadata_new"
    source_year = "2024"
    source_month = "04"  # April
    result = process_metadata(spark, source_tables, target_table, source_year, source_month)
    result.show()
