from pyspark.sql.functions import current_timestamp, col, regexp_extract, lower
from pyspark.sql.utils import AnalysisException, ParseException
import re

def update_source_metadata(tables_to_append):
    try:
        # Read source metadata
        source_metadata_df = spark.table(f"{source_schema_name}.{source_metadata_table}")
        
        # Extract year and month from file_name column
        source_metadata_df = source_metadata_df.withColumn(
            "extracted_year", regexp_extract(col("file_name"), r"/(\d{4})/", 1)
        ).withColumn(
            "extracted_month", regexp_extract(col("file_name"), r"/(\d{2})/McDashCore", 1)
        )
        
        # Filter rows based on the current year and month
        filtered_metadata = source_metadata_df.filter(
            (col("extracted_year") == current_year) &
            (col("extracted_month") == current_month)
        )
        
        # Create an empty DataFrame to store the results
        result_metadata = spark.createDataFrame([], filtered_metadata.schema)
        
        # For each table, filter the metadata and union the results
        for table in tables_to_append:
            # Create a case-insensitive regex pattern for the table name
            table_pattern = re.compile(f'.*{re.escape(table)}.*', re.IGNORECASE)
            
            # Use the regex pattern to filter the metadata
            table_metadata = filtered_metadata.filter(
                col("file_name").rlike(table_pattern.pattern)
            )
            
            # Union with the result DataFrame
            result_metadata = result_metadata.union(table_metadata)
        
        # Add update timestamp
        result_metadata = result_metadata.withColumn("metadata_updated_at", current_timestamp())
        
        # Append filtered metadata to target
        result_metadata.write.mode("append").saveAsTable(f"{target_metadata_schema}.{target_metadata_table}")
        
        print(f"Metadata updated in {target_metadata_schema}.{target_metadata_table}")
        print(f"Filtered for year: {current_year}, month: {current_month}, and tables: {', '.join(tables_to_append)}")
    except AnalysisException as e:
        print(f"Error: Metadata table not found or invalid schema - {str(e)}")
        raise
    except ParseException as e:
        print(f"Error: Invalid SQL or DataFrame operation in metadata update - {str(e)}")
        raise
    except Exception as e:
        print(f"Unexpected error occurred while updating metadata: {str(e)}")
        raise

# ... [rest of the code remains the same]

# Perform append operation for each table in the list
for table in tables_to_append:
    try:
        append_and_update_metadata(table)
    except Exception as e:
        print(f"Failed to process table {table}: {str(e)}")
        dbutils.notebook.exit(f"Failed to process table {table}")

# Update source metadata
try:
    update_source_metadata(tables_to_append)
except Exception as e:
    print(f"Failed to update source metadata: {str(e)}")
    dbutils.notebook.exit("Failed to update source metadata")

print("All append operations and metadata updates completed successfully.")
dbutils.notebook.exit("SUCCESS")
