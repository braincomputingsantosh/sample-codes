def append_and_update_metadata(table_name):
    try:
        # Construct full table names
        source_table = f"{source_schema_name}.{table_name}"
        target_table = f"{target_schema_name}.{table_name}"
        
        # Read the source table
        source_df = spark.table(source_table)
        
        # Extract year and month from source_metadata
        from pyspark.sql.functions import from_json, col
        
        source_df = source_df.withColumn("metadata", from_json(col("source_metadata"), "MAP<STRING,STRING>"))
        file_path = source_df.select(col("metadata.file_path")).first()[0]
        
        import re
        year_month_match = re.search(r'/(\d{4})/(\d{2})/', file_path)
        if year_month_match:
            year, month = year_month_match.groups()
        else:
            raise ValueError("Year and month not found in file path")
        
        # Check if data for this year and month exists in target table
        target_df = spark.table(target_table)
        existing_data = target_df.filter((col("year") == year) & (col("month") == month))
        
        if existing_data.count() == 0:
            # Data doesn't exist, proceed with append
            source_df.write.mode("append").saveAsTable(target_table)
            
            print(f"Append operation completed for {table_name}")
            print(f"Source: {source_table}")
            print(f"Target: {target_table}")
            return "PROCESSED"
        else:
            print(f"Data for year {year} and month {month} already exists in {target_table}. Skipping append.")
            return "SKIPPED"
        
    except Exception as e:
        error_message = f"Error appending {table_name}: {str(e)}\n{traceback.format_exc()}"
        print(error_message)
        return "FAILED"
