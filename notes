from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import StructType, StructField, StringType
import xml.etree.ElementTree as ET

# Initialize Spark Session
spark = SparkSession.builder.appName("ManualXMLParsing").getOrCreate()

# Path to your XML file
xml_file_path = "/path/to/STRUCTURE_CUT_ATTRIBUTES.xml"

# Read the XML file as text
xml_df = spark.read.text(xml_file_path)

# Define the schema for the parsed XML
schema = StructType([
    StructField("nm_lgl", StringType(), True),
    StructField("nm_lgl_short", StringType(), True)
])

# UDF to parse the XML string
def parse_xml(xml_string):
    try:
        root = ET.fromstring(xml_string)
        nm_lgl = root.find('nm_lgl').text if root.find('nm_lgl') is not None else None
        nm_lgl_short = root.find('nm_lgl_short').text if root.find('nm_lgl_short') is not None else None
        return (nm_lgl, nm_lgl_short)
    except ET.ParseError:
        return (None, None)

parse_xml_udf = udf(parse_xml, schema)

# Apply the UDF to transform the DataFrame
transformed_df = xml_df.select(parse_xml_udf("value").alias("parsed")).select("parsed.*")

# Show the DataFrame to verify the data
transformed_df.show()

# Write the DataFrame to a Delta table in Databricks
delta_table_path = "/mnt/delta/xml_data_table"
transformed_df.write.format("delta").mode("overwrite").save(delta_table_path)

# Optionally, you can create a Delta table reference in the Databricks metastore
# spark.sql(f"CREATE TABLE my_xml_data_table USING DELTA LOCATION '{delta_table_path}'")

# Stop the Spark session
spark.stop()
