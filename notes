# Databricks notebook source
from pyspark.sql import SparkSession
from pyspark.dbutils import DBUtils
import psycopg2

# COMMAND ----------

# Create widgets for user input
dbutils.widgets.text("host", "", "Host")
dbutils.widgets.text("port", "5432", "Port")
dbutils.widgets.text("database", "", "Database")
dbutils.widgets.text("user", "", "User")
dbutils.widgets.text("password", "", "Password")

# COMMAND ----------

# Retrieve widget values
host = dbutils.widgets.get("host")
port = dbutils.widgets.get("port")
database = dbutils.widgets.get("database")
user = dbutils.widgets.get("user")
password = dbutils.widgets.get("password")

# COMMAND ----------

# Function to test the connection
def test_connection(host, port, database, user, password):
    try:
        conn = psycopg2.connect(
            host=host,
            port=port,
            database=database,
            user=user,
            password=password
        )
        conn.close()
        return True
    except Exception as e:
        print(f"Connection error: {str(e)}")
        return False

# COMMAND ----------

# Test the connection and display result
if test_connection(host, port, database, user, password):
    print("Connection successful!")
    
    # Example: Create a DataFrame from a PostgreSQL table
    jdbc_url = f"jdbc:postgresql://{host}:{port}/{database}"
    table_name = "your_table_name"  # Replace with an actual table name
    
    df = spark.read \
        .format("jdbc") \
        .option("url", jdbc_url) \
        .option("dbtable", table_name) \
        .option("user", user) \
        .option("password", password) \
        .load()
    
    print(f"Successfully loaded data from table '{table_name}'")
    df.show(5)  # Display first 5 rows of the DataFrame
else:
    print("Connection failed. Please check your connection details and try again.")

# COMMAND ----------

# Clean up widgets
dbutils.widgets.removeAll()
