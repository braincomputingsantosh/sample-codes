# HELOC Exam Process Task Requirements

## 1. Overview
Enhance the existing data append function to prevent duplicate entries in the target table based on year and month, and update specific columns in the target table.

## 2. Functional Requirements

2.1. Source Data Extraction
- Extract the file path from the `source_metadata` JSON column in the source table (bk_mpo_raw).
- Parse the year and month from the file path (format: /Volumes/staging/bkmpo/raw/YYYY/MM/filename.csv).

2.2. Target Table Verification
- Check if data for the extracted year and month already exists in the target table.
- The target table must have 'year' and 'month' columns for comparison.

2.3. Conditional Data Append
- If data for the specific year and month does not exist in the target table, append the new data.
- If data already exists, do not append and proceed to update specific columns.

2.4. Target Table Update
- For the HELOC target table, update the following columns:
  - `ingest_time`: Set to the current timestamp when the operation is performed.
  - `file_path`: Update with the file path of the source data being processed.

2.5. Logging and Reporting
- Log the result of each operation (PROCESSED or FAILED).
- For PROCESSED results, include details of the updated `ingest_time` and `file_path`.
- Provide detailed error messages in case of failures.

## 3. Technical Requirements

3.1. Use PySpark for data processing.
3.2. Implement error handling and exception reporting.
3.3. Ensure compatibility with the existing Databricks environment.

## 4. Performance Considerations

4.1. Optimize the data existence check to minimize processing time.
4.2. Consider the impact on overall ETL pipeline performance.
4.3. Ensure efficient updating of `ingest_time` and `file_path` columns in the target table.
