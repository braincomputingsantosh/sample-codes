from pathlib import Path
import re
import os

def extract_file_info(file_path):
    path = Path(file_path)
    
    # Extract file name and extension
    file_name = path.name
    file_extension = path.suffix.lstrip('.')
    
    # Extract year and month using regex
    match = re.search(r'/(\d{4})/(\d{2})/', str(path))
    year, month = match.groups() if match else (None, None)
    
    # Get file size
    file_size = os.path.getsize(file_path) if os.path.exists(file_path) else None
    
    return {
        "file_name": file_name,
        "file_path": str(path),
        "file_type": file_extension,
        "year": year,
        "month": month,
        "file_size": file_size
    }


from pyspark.sql import SparkSession
from pyspark.sql.functions import current_timestamp, lit

def process_and_update_metadata(spark, metadata_table_name, file_path, project_name):
    # Extract file info
    file_info = extract_file_info(file_path)
    
    # Prepare metadata dictionary
    metadata = {
        "project_name": project_name,
        "file_name": file_info["file_name"],
        "file_path": file_info["file_path"],
        "process_date": current_timestamp().cast("string"),
        "file_type": file_info["file_type"],
        "file_process_state": "PROCESSED",  # You might want to parameterize this
        "rows_inserted": None,  # This should be updated with actual value after processing
        "file_size": file_info["file_size"],
        "last_error_msg": "",
        "reprocess": "N"
    }
    
    # Update metadata
    update_source_metadata(spark, metadata_table_name, metadata)

def update_source_metadata(spark, metadata_table_name, metadata):
    # (Keep the same implementation as in the previous answer)
    update_sql = f"""
    MERGE INTO {metadata_table_name} AS target
    USING (SELECT 
        '{metadata.get("project_name")}' AS project_name,
        '{metadata.get("file_name")}' AS file_name,
        '{metadata.get("file_path")}' AS file_path,
        '{metadata.get("process_date", current_timestamp().cast("string"))}' AS process_date,
        '{metadata.get("file_type")}' AS file_type,
        '{metadata.get("file_process_state")}' AS file_process_state,
        {metadata.get("rows_inserted", 'NULL')} AS rows_inserted,
        {metadata.get("file_size", 'NULL')} AS file_size,
        '{metadata.get("last_error_msg", "")}' AS last_error_msg,
        '{metadata.get("reprocess", "N")}' AS reprocess
    ) AS source
    ON target.file_path = source.file_path
    WHEN MATCHED THEN
        UPDATE SET
            project_name = source.project_name,
            file_name = source.file_name,
            process_date = source.process_date,
            file_type = source.file_type,
            file_process_state = source.file_process_state,
            rows_inserted = source.rows_inserted,
            file_size = source.file_size,
            last_error_msg = source.last_error_msg,
            reprocess = source.reprocess
    WHEN NOT MATCHED THEN
        INSERT (project_name, file_name, file_path, process_date, file_type, file_process_state, rows_inserted, file_size, last_error_msg, reprocess)
        VALUES (source.project_name, source.file_name, source.file_path, source.process_date, source.file_type, source.file_process_state, source.rows_inserted, source.file_size, source.last_error_msg, source.reprocess)
    """
    
    spark.sql(update_sql)
    print(f"Metadata updated for file: {metadata.get('file_name')}")

# Example usage
if __name__ == "__main__":
    spark = SparkSession.builder.getOrCreate()
    
    file_path = "/Volumes/staging/bk_mpo_raw/landing/heloc/2024/04/HELOCData_202404_ALL/HELOCData_202404_ALL_2024-06-11.csv"
    project_name = "HELOC"
    metadata_table_name = "your_database.source_metadata"
    
    process_and_update_metadata(spark, metadata_table_name, file_path, project_name)
