# Import required libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

def migrate_data(source_schema, source_table, target_schema, target_table, num_rows):
    # Create SparkSession
    spark = SparkSession.builder.appName("DataMigration").getOrCreate()

    # Set the database
    spark.sql(f"USE {source_schema}")

    # Read the source table
    df = spark.table(f"{source_schema}.{source_table}")

    # Sample the specified number of rows
    sampled_df = df.limit(num_rows)

    # Create the target schema if it doesn't exist
    spark.sql(f"CREATE SCHEMA IF NOT EXISTS {target_schema}")

    # Create the target table with the same schema as the source table
    sampled_df.write.mode("overwrite").saveAsTable(f"{target_schema}.{target_table}")

    print(f"Successfully migrated {num_rows} rows from {source_schema}.{source_table} to {target_schema}.{target_table}")

# Set your parameters
source_schema = dbutils.widgets.get("source_schema")
source_table = dbutils.widgets.get("source_table")
target_schema = dbutils.widgets.get("target_schema")
target_table = dbutils.widgets.get("target_table")
num_rows = int(dbutils.widgets.get("num_rows"))

# Call the function
migrate_data(source_schema, source_table, target_schema, target_table, num_rows)
