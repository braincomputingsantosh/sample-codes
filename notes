-- SQL Version
-- Option 1: Using EXCEPT
(SELECT * FROM bk_mpo_raw
EXCEPT
SELECT * FROM bk_mpo_silver)
UNION ALL
(SELECT * FROM bk_mpo_silver
EXCEPT
SELECT * FROM bk_mpo_raw);

-- Option 2: Using LEFT JOIN (if you want to see which specific rows are different)
SELECT 
  COALESCE(r.*, s.*) AS all_columns,
  CASE 
    WHEN r.* IS NULL THEN 'Only in Silver'
    WHEN s.* IS NULL THEN 'Only in Raw'
    ELSE 'Differ'
  END AS difference_type
FROM bk_mpo_raw r
FULL OUTER JOIN bk_mpo_silver s
ON r.id = s.id  -- Replace 'id' with your actual primary key column
WHERE r.* IS NULL OR s.* IS NULL OR r.* != s.*;

# PySpark Version
from pyspark.sql.functions import col, when

# Read the tables
raw_df = spark.table("bk_mpo_raw")
silver_df = spark.table("bk_mpo_silver")

# Option 1: Using except and union
diff_df = raw_df.exceptAll(silver_df).unionAll(silver_df.exceptAll(raw_df))

# Option 2: Using join
# Assuming 'id' is the primary key, replace it with your actual primary key column
columns = raw_df.columns
join_condition = (raw_df['id'] == silver_df['id'])

diff_df = raw_df.join(silver_df, join_condition, "full_outer").select(
    *[when(raw_df[c].isNull() & silver_df[c].isNotNull(), silver_df[c])
      .when(raw_df[c].isNotNull() & silver_df[c].isNull(), raw_df[c])
      .otherwise(raw_df[c]).alias(c) for c in columns],
    when(raw_df['id'].isNull(), 'Only in Silver')
    .when(silver_df['id'].isNull(), 'Only in Raw')
    .otherwise('Differ').alias('difference_type')
).where(
    (raw_df['id'].isNull()) | 
    (silver_df['id'].isNull()) | 
    (raw_df['id'] != silver_df['id'])
)

# Show the differences
diff_df.show()




1. Time Travel (Delta Lake tables):
   If your table is a Delta Lake table, you can use Time Travel to access the data before the TRUNCATE operation:

   -- SQL
   SELECT * FROM table_name VERSION AS OF version
   -- or
   SELECT * FROM table_name TIMESTAMP AS OF timestamp

   # PySpark
   from delta.tables import *
   
   deltaTable = DeltaTable.forName(spark, "table_name")
   df = deltaTable.history()
   df.show()  # Find the version number before TRUNCATE
   
   # Read the version before TRUNCATE
   df_before_truncate = spark.read.format("delta").option("versionAsOf", version_number).table("table_name")

2. VACUUM command considerations:
   -- Check the retention period
   DESCRIBE HISTORY table_name;
   
   -- Extend retention period if needed
   ALTER TABLE table_name SET TBLPROPERTIES (delta.logRetentionDuration = '30 DAYS');

3. Databricks Backup and Restore:
   -- This is an admin-level feature, no direct SQL/PySpark commands

4. Snapshots (if implemented):
   -- Restore from snapshot (implementation-specific)

5. Logging and Audit:
   -- Query audit logs (admin access required)
   SELECT * FROM system.access_log
   WHERE table_name = 'your_table'
   AND operation = 'TRUNCATE'
   ORDER BY event_time DESC
   LIMIT 1;

6. External Backups:
   -- Restore from external backup (implementation-specific)

7. Reconstructing data:
   -- Use source data or downstream copies to reconstruct (implementation-specific)



-- Revoke TRUNCATE permission from a specific user
REVOKE TRUNCATE ON TABLE catalog_name.schema_name.table_name FROM `user_name`;

-- Revoke TRUNCATE permission from a specific group
REVOKE TRUNCATE ON TABLE catalog_name.schema_name.table_name FROM `group_name`;

-- If you want to revoke ALL permissions and then grant specific ones (excluding TRUNCATE)
REVOKE ALL PRIVILEGES ON TABLE catalog_name.schema_name.table_name FROM `user_or_group_name`;

GRANT SELECT, INSERT, UPDATE, DELETE ON TABLE catalog_name.schema_name.table_name TO `user_or_group_name`;



# Strategy 1: Row count comparison
# PostgreSQL
postgres_count = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:postgresql://host:port/database") \
    .option("dbtable", "acs_table") \
    .option("user", "username") \
    .option("password", "password") \
    .load() \
    .count()

# Databricks
databricks_count = spark.table("acs_table").count()

print(f"PostgreSQL count: {postgres_count}")
print(f"Databricks count: {databricks_count}")

# Strategy 2: Checksum comparison
from pyspark.sql.functions import sum, hash

# PostgreSQL
postgres_checksum = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:postgresql://host:port/database") \
    .option("dbtable", "acs_table") \
    .option("user", "username") \
    .option("password", "password") \
    .load() \
    .agg(sum(hash(*columns)).alias("checksum")) \
    .collect()[0]["checksum"]

# Databricks
databricks_checksum = spark.table("acs_table") \
    .agg(sum(hash(*columns)).alias("checksum")) \
    .collect()[0]["checksum"]

print(f"PostgreSQL checksum: {postgres_checksum}")
print(f"Databricks checksum: {databricks_checksum}")

# Strategy 3: Detailed comparison
from pyspark.sql.functions import col

postgres_df = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:postgresql://host:port/database") \
    .option("dbtable", "acs_table") \
    .option("user", "username") \
    .option("password", "password") \
    .load()

databricks_df = spark.table("acs_table")

diff_df = postgres_df.exceptAll(databricks_df).union(databricks_df.exceptAll(postgres_df))

diff_df.show()
print(f"Number of different rows: {diff_df.count()}")
