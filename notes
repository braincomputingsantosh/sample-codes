import datetime
from pyspark.sql.functions import col, from_json, lit
from pyspark.sql.types import StructType, StructField, StringType, LongType

class ETLProcessor:
    def __init__(self, spark, source_schema, target_schema, metadata_table_name):
        self.spark = spark
        self.source_schema = source_schema
        self.target_schema = target_schema
        self.metadata_table_name = metadata_table_name

    def truncate_and_insert_table_data(self, tables):
        results = []
        for table_name in tables:
            try:
                source_table = f"{self.source_schema}.{table_name}"
                target_table = f"{self.target_schema}.{table_name}"

                # Truncate the target table
                self.spark.sql(f"TRUNCATE TABLE {target_table}")

                # Read the source table
                source_df = self.spark.table(source_table)

                # Define the schema for the source_metadata column
                metadata_schema = StructType([
                    StructField("file_path", StringType(), True),
                    StructField("file_name", StringType(), True),
                    StructField("file_size", LongType(), True)
                ])

                # Extract metadata from the first row
                first_row_metadata = source_df.select(
                    from_json(col("source_metadata"), metadata_schema).alias("metadata")
                ).first()

                if first_row_metadata:
                    file_path = first_row_metadata.metadata.file_path
                    file_name = first_row_metadata.metadata.file_name
                    file_size = first_row_metadata.metadata.file_size
                else:
                    raise ValueError(f"No metadata found for table {table_name}")

                # Insert all records into the target table
                source_df.write.mode("overwrite").saveAsTable(target_table)

                rows_inserted = source_df.count()

                print(f"Truncate and insert operation completed for {table_name}")
                print(f"Source: {source_table}")
                print(f"Target: {target_table}")
                results.append((table_name, "SUCCESS", file_path, file_name, file_size, rows_inserted))
            except Exception as e:
                error_message = f"Error in truncate and insert operation for {table_name}: {str(e)}"
                print(error_message)
                results.append((table_name, "FAILED", None, None, None, 0))

        return results

    def run_etl(self, tables_to_process):
        insert_results = self.truncate_and_insert_table_data(tables_to_process)
        overall_status = "SUCCESS" if all(status == "SUCCESS" for _, status, _, _, _, _ in insert_results) else "FAILED"
        
        # Process and update metadata for each table
        for table_name, status, file_path, file_name, file_size, rows_inserted in insert_results:
            if file_path:
                project_name = f"{table_name}_bk_mpo_full_load"
                self.process_and_update_metadata(
                    file_path=file_path,
                    file_name=file_name,
                    file_size=file_size,
                    project_name=project_name,
                    process_state=status,
                    rows_inserted=rows_inserted,
                    reprocess=False
                )

        print(f"Overall status: {overall_status}")
        return insert_results, overall_status

    def update_source_metadata(self, metadata):
        update_sql = f"""
        INSERT INTO {self.metadata_table_name} 
        (project_name, file_name, file_path, process_date, file_type, file_process_state, rows_inserted, file_size, last_error_msg, reprocess)
        VALUES (
            '{metadata.get("project_name")}',
            '{metadata.get("file_name")}',
            '{metadata.get("file_path")}',
            '{metadata.get("process_date")}',
            '{metadata.get("file_type")}',
            '{metadata.get("file_process_state")}',
            {metadata.get("rows_inserted", "NULL")},
            {metadata.get("file_size", "NULL")},
            '{metadata.get("last_error_msg", "")}',
            {metadata.get("reprocess", "false")}
        )
        """
        self.spark.sql(update_sql)
        print(f"Metadata inserted for file: {metadata.get('file_name')}")

    def process_and_update_metadata(self, file_path, file_name, file_size, project_name, process_state, rows_inserted, reprocess):
        _, file_extension = os.path.splitext(file_name)
        
        metadata = {
            "project_name": project_name,
            "file_name": file_name,
            "file_path": file_path,
            "process_date": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "file_type": file_extension.lstrip('.'),
            "file_process_state": process_state,
            "rows_inserted": rows_inserted,
            "file_size": file_size,
            "last_error_msg": "",
            "reprocess": reprocess
        }

        self.update_source_metadata(metadata)

# Usage example
spark = SparkSession.builder.appName("ETLProcessor").getOrCreate()
etl_processor = ETLProcessor(spark, 
                             source_schema="source_schema", 
                             target_schema="target_schema", 
                             metadata_table_name="test_bk_mpo.source_metadata_new")

tables_to_process = ["table1", "table2", "table3"]
results, overall_status = etl_processor.run_etl(tables_to_process)

print(f"ETL Process completed with status: {overall_status}")
for table, status, file_path, file_name, file_size, rows_inserted in results:
    print(f"Table: {table}, Status: {status}, File: {file_name}, Rows Inserted: {rows_inserted}")
