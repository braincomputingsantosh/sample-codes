# Databricks notebook source
# MAGIC %md
# MAGIC # Merge (Upsert) Operation Notebook with Metadata Update (Job Version)

# COMMAND ----------

# Job parameters
dbutils.widgets.text("current_year", "2024", "Current Year")
dbutils.widgets.text("current_month", "07", "Current Month")

current_year = dbutils.widgets.get("current_year")
current_month = dbutils.widgets.get("current_month")

# List of tables that need merge operation
tables_to_merge = ['customer_data', 'transaction_history', 'product_catalog']

# Source schema name
source_schema_name = 'bk_hpi_raw'

# Target schema name
target_schema_name = 'bk_hpi_processed'

# Metadata schemas and tables
source_metadata_table = 'source_metadata'
target_metadata_schema = 'bk_mpo'
target_metadata_table = 'source_meta_data'

print(f"Job will process data for Year: {current_year}, Month: {current_month}")

# COMMAND ----------

from pyspark.sql.functions import current_timestamp, col, regexp_extract
from pyspark.sql.utils import AnalysisException, ParseException
from delta.tables import DeltaTable

def merge_records(table_name):
    try:
        # Construct full table names
        source_table = f"{source_schema_name}.{table_name}"
        target_table = f"{target_schema_name}.{table_name}"
        
        # Read the source table
        source_df = spark.table(source_table)
        
        # Assuming the target table is a Delta table
        target_delta_table = DeltaTable.forName(spark, target_table)
        
        # Perform the merge operation
        (target_delta_table.alias("target")
         .merge(
             source_df.alias("source"),
             "target.id = source.id"  # Assume 'id' is the matching key
         )
         .whenMatchedUpdateAll()
         .whenNotMatchedInsertAll()
         .execute())
        
        print(f"Merge operation completed for {table_name}")
        print(f"Source: {source_table}")
        print(f"Target: {target_table}")
    except AnalysisException as e:
        print(f"Error: Table not found or invalid schema - {str(e)}")
        raise
    except ParseException as e:
        print(f"Error: Invalid SQL or DataFrame operation - {str(e)}")
        raise
    except Exception as e:
        print(f"Unexpected error occurred while merging {table_name}: {str(e)}")
        raise

def update_source_metadata():
    try:
        # Read source metadata
        source_metadata_df = spark.table(f"{source_schema_name}.{source_metadata_table}")
        
        # Extract year and month from file_name column
        source_metadata_df = source_metadata_df.withColumn(
            "extracted_year", regexp_extract(col("file_name"), r"/(\d{4})/", 1)
        ).withColumn(
            "extracted_month", regexp_extract(col("file_name"), r"/(\d{2})/McDashCore", 1)
        )
        
        # Filter rows based on the current year and month
        filtered_metadata = source_metadata_df.filter(
            (col("extracted_year") == current_year) &
            (col("extracted_month") == current_month)
        )
        
        # Read target metadata table
        target_metadata_df = spark.table(f"{target_metadata_schema}.{target_metadata_table}")
        
        # Append filtered metadata to target
        updated_metadata = target_metadata_df.union(filtered_metadata.drop("extracted_year", "extracted_month"))
        
        # Write updated metadata back to target table
        updated_metadata.write.mode("overwrite").saveAsTable(f"{target_metadata_schema}.{target_metadata_table}")
        
        print(f"Metadata updated in {target_metadata_schema}.{target_metadata_table}")
        print(f"Filtered for year: {current_year}, month: {current_month}")
    except AnalysisException as e:
        print(f"Error: Metadata table not found or invalid schema - {str(e)}")
        raise
    except ParseException as e:
        print(f"Error: Invalid SQL or DataFrame operation in metadata update - {str(e)}")
        raise
    except Exception as e:
        print(f"Unexpected error occurred while updating metadata: {str(e)}")
        raise

# COMMAND ----------

# Perform merge operation for each table in the list
for table in tables_to_merge:
    try:
        merge_records(table)
    except Exception as e:
        print(f"Failed to process table {table}: {str(e)}")
        dbutils.notebook.exit(f"Failed to process table {table}")

# Update source metadata
try:
    update_source_metadata()
except Exception as e:
    print(f"Failed to update source metadata: {str(e)}")
    dbutils.notebook.exit("Failed to update source metadata")

# COMMAND ----------

print("All merge operations and metadata updates completed successfully.")
dbutils.notebook.exit("SUCCESS")
