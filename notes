from pyspark.sql import SparkSession
from pyspark.sql.functions import lit
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_replica_table(spark, source_schema, source_table, target_schema, target_table):
    logger.info(f"Starting to create replica of {source_schema}.{source_table} in {target_schema}.{target_table}")
    
    try:
        # Read the schema of the source table
        source_df = spark.table(f"{source_schema}.{source_table}")
        
        # Get the schema as a string
        schema_string = ', '.join([f"`{field.name}` {field.dataType.simpleString()}" for field in source_df.schema.fields])
        
        # Create the new table with the same schema
        create_table_query = f"""
        CREATE TABLE IF NOT EXISTS {target_schema}.{target_table} (
            {schema_string}
        ) USING DELTA
        """
        
        spark.sql(create_table_query)
        
        logger.info(f"Successfully created table {target_schema}.{target_table}")
        
        # Verify the new table
        new_df = spark.table(f"{target_schema}.{target_table}")
        logger.info("New table schema:")
        new_df.printSchema()
        
    except Exception as e:
        logger.error(f"Error creating replica table: {str(e)}")

# Example usage
if __name__ == "__main__":
    spark = SparkSession.builder.appName("CreateReplicaTable").getOrCreate()
    
    source_schema = "bk_mpo"
    source_table = "source_metadata_new"
    target_schema = "test_bk_mpo"
    target_table = "source_metadata_new"
    
    create_replica_table(spark, source_schema, source_table, target_schema, target_table)
