from pyspark.sql.functions import col, lit, current_timestamp
from datetime import datetime
import traceback

def update_source_metadata(spark, metadata_table_name, metadata):
    update_sql = f"""
    MERGE INTO {metadata_table_name} AS target
    USING (SELECT 
        CAST(UNIX_TIMESTAMP() * 1000000 AS BIGINT) AS id,
        '{metadata.get("project_name")}' AS project_name,
        '{metadata.get("file_name")}' AS file_name,
        '{metadata.get("file_path")}' AS file_path,
        '{metadata.get("process_date")}' AS process_date,
        '{metadata.get("file_type")}' AS file_type,
        '{metadata.get("file_process_state")}' AS file_process_state,
        {metadata.get("rows_inserted", 'NULL')} AS rows_inserted,
        {metadata.get("file_size", 'NULL')} AS file_size,
        '{metadata.get("last_error_msg", "")}' AS last_error_msg,
        '{metadata.get("reprocess", "N")}' AS reprocess
    ) AS source
    ON target.file_path = source.file_path
    WHEN MATCHED THEN
        UPDATE SET
            project_name = source.project_name,
            file_name = source.file_name,
            process_date = source.process_date,
            file_type = source.file_type,
            file_process_state = source.file_process_state,
            rows_inserted = source.rows_inserted,
            file_size = source.file_size,
            last_error_msg = source.last_error_msg,
            reprocess = source.reprocess
    WHEN NOT MATCHED THEN
        INSERT (id, project_name, file_name, file_path, process_date, file_type, file_process_state, rows_inserted, file_size, last_error_msg, reprocess)
        VALUES (source.id, source.project_name, source.file_name, source.file_path, source.process_date, source.file_type, source.file_process_state, source.rows_inserted, source.file_size, source.last_error_msg, source.reprocess)
    """
    
    spark.sql(update_sql)
    print(f"Metadata updated for file: {metadata.get('file_name')}")

def extract_file_info(file_path):
    # Implement this function to extract relevant file information
    # For now, we'll return a dummy dictionary
    return {
        "file_name": file_path.split("/")[-1],
        "file_type": "csv",  # Assume CSV for now
        "file_size": 1000  # Dummy size
    }

def append_and_update_metadata(spark, raw_table_name, silver_table_name, metadata_table_name, as_of_month):
    try:
        # Construct full table names
        raw_table = f"bk_mpo_raw.{raw_table_name}"
        silver_table = f"bk_mpo_silver.{silver_table_name}"
        
        # Read the raw table
        raw_df = spark.table(raw_table)
        
        # Extract file_path from source_metadata in the raw table
        file_path = raw_df.filter(col("as_of_month") == as_of_month) \
                          .select(col("source_metadata.file_path")).first()[0]
        
        # Check if data for this as_of_month exists in silver table
        silver_df = spark.table(silver_table)
        existing_data = silver_df.filter(col("as_of_month") == as_of_month)
        
        if existing_data.count() == 0:
            # Data doesn't exist in silver table, proceed with append
            columns_to_append = [col for col in raw_df.columns if col != "source_metadata"]
            data_to_append = raw_df.filter(col("as_of_month") == as_of_month) \
                                   .select(columns_to_append) \
                                   .withColumn("file_path", lit(file_path)) \
                                   .withColumn("ingest_time", current_timestamp())
            
            # Append the data to the silver table
            data_to_append.write.mode("append").saveAsTable(silver_table)
            
            rows_inserted = data_to_append.count()
            process_state = "PROCESSED"
        else:
            # Data exists in silver table, update file_path and ingest_time
            silver_df.filter(col("as_of_month") == as_of_month) \
                     .update({"file_path": file_path, "ingest_time": current_timestamp()})
            
            rows_inserted = 0
            process_state = "UPDATED"
        
        # Extract file info
        file_info = extract_file_info(file_path)
        
        # Prepare metadata
        metadata = {
            "project_name": "bk_mpo",
            "file_name": file_info["file_name"],
            "file_path": file_path,
            "process_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "file_type": file_info["file_type"],
            "file_process_state": process_state,
            "rows_inserted": rows_inserted,
            "file_size": file_info["file_size"],
            "last_error_msg": "",
            "reprocess": "N"
        }
        
        # Update metadata
        update_source_metadata(spark, metadata_table_name, metadata)
        
        print(f"Processing completed for {silver_table_name}, as_of_month: {as_of_month}")
        return "PROCESSED"
    
    except Exception as e:
        error_message = f"Error processing {raw_table_name} to {silver_table_name}: {str(e)}\n{traceback.format_exc()}"
        
        # Update metadata with error information
        metadata = {
            "project_name": "bk_mpo",
            "file_name": file_path.split("/")[-1],
            "file_path": file_path,
            "process_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "file_type": "unknown",
            "file_process_state": "FAILED",
            "rows_inserted": 0,
            "file_size": 0,
            "last_error_msg": error_message[:255],  # Truncate error message if necessary
            "reprocess": "Y"
        }
        update_source_metadata(spark, metadata_table_name, metadata)
        
        print(error_message)
        return "FAILED"

# Function to process the table
def process_table(spark, raw_table_name, silver_table_name, metadata_table_name):
    # Execute the EXCEPT query
    diff_df = spark.sql(f"""
    SELECT as_of_month FROM bk_mpo_raw.{raw_table_name}
    EXCEPT
    SELECT as_of_month FROM bk_mpo_silver.{silver_table_name}
    """)
    
    # Get the list of as_of_month values to process
    as_of_months_to_process = [row.as_of_month for row in diff_df.collect()]
    
    results = []
    for as_of_month in as_of_months_to_process:
        result = append_and_update_metadata(spark, raw_table_name, silver_table_name, metadata_table_name, as_of_month)
        results.append((as_of_month, result))
    
    return results

# Example usage
raw_table_name = "heloc"
silver_table_name = "heloc"
metadata_table_name = "bk_mpo_raw.source_metadata"
processing_results = process_table(spark, raw_table_name, silver_table_name, metadata_table_name)

for as_of_month, result in processing_results:
    print(f"as_of_month: {as_of_month}, Result: {result}")
