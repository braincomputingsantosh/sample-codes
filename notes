from pyspark.sql.functions import current_timestamp, col, regexp_extract, lit, when, udf
from pyspark.sql.types import StringType
from datetime import datetime

def update_source_metadata(tables_to_append, append_results):
    try:
        # Read source metadata
        source_metadata_df = spark.table(f"{source_schema_name}.{source_metadata_table}")
        
        # Filter rows based on the tables_to_append
        filtered_metadata = source_metadata_df.filter(
            col("file_name").rlike("|".join(tables_to_append))
        )
        
        # Create a dictionary to map table names to their process states
        table_status = {table: status for table, status in append_results}
        
        def get_process_state(file_name):
            for table, status in table_status.items():
                if table.lower() in file_name.lower():
                    return status
            return "UNKNOWN"
        
        get_process_state_udf = udf(get_process_state, StringType())
        
        # Add new columns and transform existing ones
        updated_metadata = filtered_metadata.select(
            col("id"),  # Use existing id instead of generating a new one
            lit("bk_mpo").alias("project_name"),
            col("file_name"),
            col("file_path"),
            current_timestamp().alias("process_date"),
            regexp_extract(col("file_name"), r"\.(\w+)$", 1).alias("file_type"),
            get_process_state_udf(col("file_name")).alias("file_process_state"),
            current_timestamp().alias("process_start_dtm"),
            current_timestamp().alias("process_end_dtm"),
            col("file_block_length").alias("file_size")
        )
        
        # Merge the updated metadata into the target table
        target_table = DeltaTable.forName(spark, f"{target_metadata_schema}.source_metadata_new")
        
        (target_table.alias("target")
         .merge(
             updated_metadata.alias("updates"),
             "target.id = updates.id"
         )
         .whenMatchedUpdateAll()
         .whenNotMatchedInsertAll()
         .execute())
        
        # Log processing results
        success_count = updated_metadata.filter(col("file_process_state") == "PROCESSED").count()
        failed_count = updated_metadata.filter(col("file_process_state") == "FAILED").count()
        print(f"Metadata updated in {target_metadata_schema}.source_metadata_new")
        print(f"Processed tables: {', '.join(tables_to_append)}")
        print(f"Successfully processed files: {success_count}")
        print(f"Failed files: {failed_count}")
        
        if failed_count > 0:
            print("Warning: Some files failed to process. Check the logs for details.")
    except Exception as e:
        print(f"Unexpected error occurred while updating metadata: {str(e)}")
        raise

# ... [rest of the code remains the same]
