# Import required libraries
from pyspark.sql import *
from pyspark.sql.functions import col, lit, current_timestamp, when, expr
from pyspark.sql.types import StringType
from datetime import datetime
import traceback

def update_source_metadata(spark, metadata_table_name, metadata):
    """
    Updates the metadata table with processing information
    """
    try:
        # Convert string to datetime if necessary
        if isinstance(metadata['process_date'], str):
            metadata['process_date'] = datetime.strptime(metadata['process_date'], "%Y-%m-%d %H:%M:%S")
        
        # Convert to DataFrame with proper error handling
        try:
            metadata_df = spark.createDataFrame([metadata])
        except Exception as e:
            print(f"Error creating metadata DataFrame: {str(e)}")
            raise
        
        # Handle reprocess column - simplified for efficiency
        metadata_df = metadata_df.withColumn(
            "reprocess",
            when(col("reprocess").isNull(), "false")
            .otherwise(col("reprocess").cast(StringType()))
        )
        
        # Optimize schema matching
        target_schema = spark.table(metadata_table_name).schema
        for field in target_schema.fields:
            if field.name in metadata_df.columns:
                metadata_df = metadata_df.withColumn(field.name, col(field.name).cast(field.dataType))
        
        # Use efficient delete operation
        spark.sql(f"""
        DELETE FROM {metadata_table_name}
        WHERE file_path = '{metadata['file_path']}'
        """)
        
        # Write with timeout handling
        metadata_df.write.mode("append").saveAsTable(metadata_table_name)
        
        print(f"Metadata updated for file: {metadata['file_name']}")
        
    except Exception as e:
        print(f"Error in update_source_metadata: {str(e)}")
        raise

def extract_file_info(file_path):
    """
    Extracts file information from the file path
    """
    return {
        "file_name": file_path.split("/")[-1],
        "file_type": "CSV"
    }

def append_and_update_metadata(spark, raw_table_name, silver_table_name, metadata_table_name, loan_id, as_of_month):
    """
    Main function to append data to silver table and update metadata with optimized resource usage
    """
    file_path = None
    try:
        # Construct table names
        raw_table = f"bk_mpo_raw_v1.{raw_table_name}"
        silver_table = f"test_bk_mpo.{silver_table_name}_silver"
        
        # Optimize raw table reading with specific columns
        file_path_df = spark.table(raw_table).select(
            "loan_id", 
            "as_of_month", 
            col("source_metadata.file_path")
        ).filter(
            (col("as_of_month") == as_of_month) & 
            (col("loan_id") == loan_id)
        ).cache()  # Cache small filtered dataset
        
        if file_path_df.count() == 0:
            raise Exception(f"No data found for loan_id {loan_id} and as_of_month {as_of_month}")
        
        file_path = file_path_df.first()[2]
        file_path_df.unpersist()  # Clear cache
        
        # Check existing data efficiently
        existing_count = spark.table(silver_table).filter(
            (col("as_of_month") == as_of_month) & 
            (col("loan_id") == loan_id)
        ).count()
        
        if existing_count == 0:
            # New record - Prepare data efficiently
            columns_to_append = [col for col in spark.table(raw_table).columns if col != 'source_metadata']
            data_to_append = spark.table(raw_table).filter(
                (col("as_of_month") == as_of_month) & 
                (col("loan_id") == loan_id)
            ).select(
                *columns_to_append
            ).withColumn(
                "file_path", lit(file_path)
            ).withColumn(
                "ingest_time", current_timestamp()
            )
            
            # Write with optimization
            data_to_append.write.mode("append").saveAsTable(silver_table)
            rows_inserted = data_to_append.count()
            process_state = "PROCESSED"
            
        else:
            # Update existing record
            spark.sql(f"""
            UPDATE {silver_table}
            SET file_path = '{file_path}', 
                ingest_time = current_timestamp()
            WHERE as_of_month = {as_of_month} 
            AND loan_id = {loan_id}
            """)
            rows_inserted = 0
            process_state = "UPDATED"
        
        # Prepare and update metadata
        file_info = extract_file_info(file_path)
        metadata = {
            "project_name": "bk_mpo",
            "file_name": file_info["file_name"],
            "file_path": file_path,
            "process_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "file_type": file_info["file_type"],
            "file_process_state": process_state,
            "rows_inserted": rows_inserted,
            "last_error_msg": "",
            "reprocess": "false"
        }
        
        update_source_metadata(spark, metadata_table_name, metadata)
        return "PROCESSED"
        
    except Exception as e:
        error_message = f"Error processing {raw_table_name} to {silver_table_name}: {str(e)}"
        print(error_message)
        
        if file_path:
            metadata = {
                "project_name": "bk_mpo",
                "file_name": file_path.split("/")[-1],
                "file_path": file_path,
                "process_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "file_type": "unknown",
                "file_process_state": "FAILED",
                "rows_inserted": 0,
                "last_error_msg": error_message[:255],
                "reprocess": "true"
            }
            try:
                update_source_metadata(spark, metadata_table_name, metadata)
            except Exception as meta_error:
                print(f"Error updating metadata: {str(meta_error)}")
        
        return "FAILED"

def process_table(spark, raw_table_name, silver_table_name, metadata_table_name):
    """
    Optimized main processing function for serverless environment
    """
    try:
        # Execute EXCEPT query with optimization
        diff_df = spark.sql(f"""
        SELECT loan_id, as_of_month 
        FROM bk_mpo_raw_v1.{raw_table_name}
        EXCEPT
        SELECT loan_id, as_of_month 
        FROM test_bk_mpo.{silver_table_name}_silver
        """).cache()  # Cache the small diff result
        
        # Process in smaller batches if needed
        batch_size = 100  # Adjust based on your data size
        records_to_process = [(row.loan_id, row.as_of_month) for row in diff_df.collect()]
        diff_df.unpersist()  # Clear cache
        
        results = []
        for i in range(0, len(records_to_process), batch_size):
            batch = records_to_process[i:i + batch_size]
            for loan_id, as_of_month in batch:
                try:
                    result = append_and_update_metadata(
                        spark, raw_table_name, silver_table_name,
                        metadata_table_name, loan_id, as_of_month
                    )
                    results.append(((loan_id, as_of_month), result))
                except Exception as e:
                    print(f"Error processing record {loan_id}, {as_of_month}: {str(e)}")
                    results.append(((loan_id, as_of_month), "FAILED"))
        
        return results
    
    except Exception as e:
        print(f"Error in process_table: {str(e)}")
        return []

# Example usage with error handling
try:
    raw_table_name = "loan_month"
    silver_table_name = "loan_month"
    metadata_table_name = "test_bk_mpo.source_metadata_new"
    
    processing_results = process_table(spark, raw_table_name, silver_table_name, metadata_table_name)
    
    # Print results with error handling
    for (loan_id, as_of_month), result in processing_results:
        print(f"as_of_month: {as_of_month}, loan_id: {loan_id}, Result: {result}")
        
except Exception as e:
    print(f"Fatal error in main execution: {str(e)}")
