from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType
from delta.tables import *
from pyspark.sql.window import Window

# Create a Spark session
spark = SparkSession.builder.appName("StreamingUpsertFixedSchema").getOrCreate()

# Create a sample Silver table if it doesn't exist
spark.sql("""
CREATE TABLE IF NOT EXISTS silver_customer_table (
  customer_id INT,
  name STRING,
  email STRING,
  updated_at TIMESTAMP
) USING DELTA
""")

# Insert some initial data
spark.sql("""
INSERT INTO silver_customer_table VALUES 
  (1, 'John Doe', 'john.doe@example.com', current_timestamp()),
  (2, 'Jane Smith', 'jane.smith@example.com', current_timestamp())
""")

# Define the schema explicitly
schema = StructType([
    StructField("customer_id", IntegerType(), True),
    StructField("name", StringType(), True),
    StructField("email", StringType(), True),
    StructField("updated_at", TimestampType(), True)
])

# Create a sample streaming source (for demonstration purposes)
def generate_stream_data():
    data = [
        (1, "John Doe", "john.updated@example.com", current_timestamp()),
        (3, "Alice Johnson", "alice.johnson@example.com", current_timestamp()),
        (2, "Jane Smith", "jane.updated@example.com", current_timestamp()),
        (4, "Bob Wilson", "bob.wilson@example.com", current_timestamp())
    ]
    return spark.createDataFrame(data, schema)

stream_data = generate_stream_data()
stream_data.createOrReplaceTempView("stream_source")

# Set up the streaming read
stream_df = spark.readStream.table("stream_source")

# Define the upsert function
def upsert_to_silver(microBatchOutputDF, batchId):
    # Create a Delta table object for the target table
    delta_table = DeltaTable.forName(spark, "silver_customer_table")

    # Perform the upsert operation
    (delta_table.alias("target")
     .merge(microBatchOutputDF.alias("source"), "target.customer_id = source.customer_id")
     .whenMatchedUpdateAll()
     .whenNotMatchedInsertAll()
     .execute())

    # Run validation after upsert
    validate_upsert(microBatchOutputDF, batchId)

# Validation function (same as before)
def validate_upsert(microBatchOutputDF, batchId):
    print(f"Validating upsert for batch {batchId}")
    # ... (rest of the validation function remains the same)

# Start the streaming query
query = (stream_df.writeStream
         .foreachBatch(upsert_to_silver)
         .outputMode("update")
         .trigger(once=True)  # For demonstration; use processingTime in production
         .start())

# Wait for the streaming job to finish
query.awaitTermination()

# Show the final results
print("Silver table after streaming upserts:")
spark.sql("SELECT * FROM silver_customer_table ORDER BY customer_id").show()

# Clean up (uncomment if you want to drop the table after the demo)
# spark.sql("DROP TABLE IF EXISTS silver_customer_table")
