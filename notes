from pyspark.sql.types import NumericType
from pyspark.sql.functions import col, count, when, isnan, isnull
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans

def compare_distributions(df, table_name):
    numeric_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, NumericType)]
    if not numeric_cols:
        return f"No numeric columns in {table_name}"
    
    print(f"Numeric columns in {table_name}: {numeric_cols}")
    
    # Check for null or NaN values
    null_counts = df.select([count(when(isnan(c) | isnull(c), c)).alias(c) for c in numeric_cols]).collect()[0]
    null_cols = [c for c, count in zip(numeric_cols, null_counts) if count > 0]
    if null_cols:
        print(f"Warning: Null or NaN values found in columns: {null_cols}")
        # Remove columns with null values
        numeric_cols = [c for c in numeric_cols if c not in null_cols]
        if not numeric_cols:
            return f"All numeric columns in {table_name} contain null or NaN values"
    
    try:
        # Cast all numeric columns to double to ensure compatibility
        for col_name in numeric_cols:
            df = df.withColumn(col_name, col(col_name).cast("double"))
        
        assembler = VectorAssembler(inputCols=numeric_cols, outputCol="features")
        df_vec = assembler.transform(df)
        
        # Check if the resulting DataFrame is empty
        if df_vec.count() == 0:
            return f"VectorAssembler resulted in an empty DataFrame for {table_name}"
        
        kmeans = KMeans(k=5, seed=1)
        model = kmeans.fit(df_vec)
        
        return model.clusterCenters()
    except Exception as e:
        print(f"Error in compare_distributions for {table_name}: {str(e)}")
        return f"Error: {str(e)}"
