from pyspark.sql import SparkSession
from pyspark.sql.functions import current_timestamp, lit, monotonically_increasing_id
from pyspark.sql.types import StructType, StructField, StringType, TimestampType
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def process_metadata(spark, source_tables):
    logger.info("Starting metadata processing")
    
    # Define the schema for the new table
    new_schema = StructType([
        StructField("id", StringType(), True),
        StructField("source_table", StringType(), True),
        StructField("file_name", StringType(), True),
        StructField("file_size", StringType(), True),
        StructField("file_path", StringType(), True),
        StructField("file_block_length", StringType(), True),
        StructField("file_type", StringType(), True),
        StructField("processed_time", TimestampType(), True)
    ])
    
    # Initialize an empty DataFrame with the new schema
    result_df = spark.createDataFrame([], new_schema)
    
    for table in source_tables:
        logger.info(f"Processing table: {table}")
        
        try:
            # Read the source table
            source_df = spark.table(f"bk_mpo_raw.{table}")
            
            # Extract metadata from the JSON column
            metadata_df = source_df.select(
                lit(table).alias("source_table"),
                spark.json.get_json_object(source_df.source_metadata, "$.file_name").alias("file_name"),
                spark.json.get_json_object(source_df.source_metadata, "$.file_size").alias("file_size"),
                spark.json.get_json_object(source_df.source_metadata, "$.file_path").alias("file_path"),
                spark.json.get_json_object(source_df.source_metadata, "$.file_block_length").alias("file_block_length"),
                lit("").alias("file_type"),  # Assuming file_type is not in the JSON, adjust if needed
                current_timestamp().alias("processed_time")
            )
            
            # Get one unique record
            unique_metadata = metadata_df.dropDuplicates(["source_table", "file_name"]).limit(1)
            
            # Append to the result DataFrame
            result_df = result_df.union(unique_metadata)
            
            logger.info(f"Processed {table} successfully")
        except Exception as e:
            logger.error(f"Error processing table {table}: {str(e)}")
    
    # Add monotonically increasing ID
    result_df = result_df.withColumn("id", monotonically_increasing_id())
    
    # Persist the data to source_metadata_new table
    try:
        result_df.write.mode("append").saveAsTable("source_metadata_new")
        logger.info("Data successfully persisted to source_metadata_new table")
    except Exception as e:
        logger.error(f"Error persisting data to source_metadata_new table: {str(e)}")
    
    logger.info("Metadata processing completed")
    return result_df

# Example usage
if __name__ == "__main__":
    spark = SparkSession.builder.appName("MetadataProcessor").getOrCreate()
    source_tables = ["LOAN_MONTH", "HELOC"]  # Add more tables as needed
    result = process_metadata(spark, source_tables)
    result.show()
