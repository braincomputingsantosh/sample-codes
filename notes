# Initialize the ETLProcessor
etl_processor = ETLProcessor(spark, 
                             source_schema="source_schema", 
                             target_schema="target_schema", 
                             metadata_table_name="metadata_table")

# Define the tables to process and the project name
tables_to_process = ["table1", "table2", "table3"]
project_name = "my_etl_project"

# Run the ETL process
results, overall_status = etl_processor.run_etl(tables_to_process, project_name)

# Print the results
print(f"ETL Process completed with status: {overall_status}")
for table, status, file_path, rows_inserted in results:
    print(f"Table: {table}, Status: {status}, Rows Inserted: {rows_inserted}")
