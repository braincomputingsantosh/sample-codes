from pyspark.sql import functions as F
from concurrent.futures import ThreadPoolExecutor
import math

# Assuming df_group is your Spark DataFrame
total_count = df_group.count()
batch_size = 1000  # Adjust this based on your data size and available memory
num_batches = math.ceil(total_count / batch_size)

d_results = {}
d_results_fail = {}
d_results_fail_META_TYPE = {}

start = perf_counter()

def process_batch(batch):
    results = []
    for row in batch:
        # Your existing column_process logic here
        result = column_process(row)
        results.append(result)
    return results

# Take all rows at once, but process in batches
all_rows = df_group.take(total_count)

with ThreadPoolExecutor(max_workers=10) as executor:
    futures = []
    for i in range(num_batches):
        start_idx = i * batch_size
        end_idx = min((i + 1) * batch_size, total_count)
        batch = all_rows[start_idx:end_idx]
        
        future = executor.submit(process_batch, batch)
        futures.append(future)

    for i, future in enumerate(futures):
        batch_results = future.result()
        
        # Process batch_results and update your result dictionaries
        for result in batch_results:
            # Update d_results, d_results_fail, d_results_fail_META_TYPE based on the result
            pass

        print(f"Processed batch {i+1}/{num_batches}")

print(f"Results: {d_results}")
print(f"Failures: {d_results_fail}")

finish = perf_counter()
print(f"It took {(finish-start)} second(s) to finish.")
