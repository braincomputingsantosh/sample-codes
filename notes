class ETLProcessor:
    def __init__(self, spark, source_schema, target_schema):
        self.spark = spark
        self.source_schema = source_schema
        self.target_schema = target_schema

    def truncate_and_insert_table_data(self, tables):
        results = []
        for table_name in tables:
            try:
                source_table = f"{self.source_schema}.{table_name}"
                target_table = f"{self.target_schema}.{table_name}"

                # Truncate the target table
                self.spark.sql(f"TRUNCATE TABLE {target_table}")

                # Read the source table
                source_df = self.spark.table(source_table)
                # Extract file_path from source_metadata
                file_path = source_df.select(col("source_metadata.file_path")).first()[0]

                # Insert all records into the target table
                source_df.write.mode("overwrite").saveAsTable(target_table)

                print(f"Truncate and insert operation completed for {table_name}")
                print(f"Source: {source_table}")
                print(f"Target: {target_table}")
                results.append((table_name, "SUCCESS"))
            except Exception as e:
                error_message = f"Error in truncate and insert operation for {table_name}: {str(e)}"
                print(error_message)
                results.append((table_name, error_message))

        return results

    def run_etl(self, tables_to_process):
        insert_results = self.truncate_and_insert_table_data(tables_to_process)
        overall_status = "SUCCESS" if all(status == "SUCCESS" for _, status in insert_results) else "FAILED"
        print(f"Overall status: {overall_status}")
        return insert_results, overall_status

    def update_source_metadata(self, spark, metadata_table_name, metadata):
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        update_sql = f"""
        MERGE INTO {metadata_table_name} AS target
        USING (SELECT 
            CAST(UNIX_TIMESTAMP() * 1000000 + CAST(RAND() * 1000000 AS BIGINT) AS BIGINT) AS id,
            '{metadata.get("project_name")}' AS project_name,
            '{metadata.get("file_name")}' AS file_name,
            '{metadata.get("file_path")}' AS file_path,
            '{metadata.get("process_date", current_time)}' AS process_date,
            '{metadata.get("file_type")}' AS file_type,
            '{metadata.get("file_process_state")}' AS file_process_state,
            {metadata.get("rows_inserted", "NULL")} AS rows_inserted,
            {metadata.get("file_size", "NULL")} AS file_size,
            '{metadata.get("last_error_msg", "")}' AS last_error_msg,
            '{metadata.get("reprocess")}' AS reprocess
        ) AS source
        ON target.file_path = source.file_path
        WHEN MATCHED THEN
            UPDATE SET
                project_name = source.project_name,
                file_name = source.file_name,
                process_date = source.process_date,
                file_type = source.file_type,
                file_process_state = source.file_process_state,
                rows_inserted = source.rows_inserted,
                file_size = source.file_size,
                last_error_msg = source.last_error_msg,
                reprocess = source.reprocess
        WHEN NOT MATCHED THEN
            INSERT (id, project_name, file_name, file_path, process_date, file_type, file_process_state, rows_inserted, file_size, last_error_msg, reprocess)
            VALUES (source.id, source.project_name, source.file_name, source.file_path, source.process_date, source.file_type, source.file_process_state, source.rows_inserted, source.file_size, source.last_error_msg, source.reprocess)
        """
        spark.sql(update_sql)
        print(f"Metadata updated for file: {metadata.get('file_name')}")

    def process_and_update_metadata(self, spark, metadata_table_name, file_path, project_name, process_state, rows_inserted, reprocess):
        # Extract file info
        file_info = extract_file_info(file_path)

        # Prepare metadata dictionary
        metadata = {
            "project_name": project_name,
            "file_name": file_info["file_name"],
            "file_path": file_info["file_path"],
            "process_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "file_type": file_info["file_type"],
            "file_process_state": process_state,
            "rows_inserted": rows_inserted,  # TODO: Insert count here
            "file_size": file_info["file_size"],
            "last_error_msg": "",
            "reprocess": reprocess
        }

        # Update source metadata
        self.update_source_metadata(spark, metadata_table_name, metadata)
