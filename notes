from pyspark.sql.functions import current_timestamp, col, regexp_extract, lit
from pyspark.sql.types import StringType
from delta.tables import DeltaTable
import traceback

def update_source_metadata(tables_to_append):
    try:
        print(f"Starting metadata update for tables: {', '.join(tables_to_append)}")
        
        # Read source metadata
        source_metadata_df = spark.table(f"{source_schema_name}.{source_metadata_table}")
        
        # Filter rows based on the tables_to_append
        table_pattern = "|".join(tables_to_append)
        filtered_metadata = source_metadata_df.filter(
            col("file_name").rlike(f"(?i){table_pattern}")
        )
        
        if filtered_metadata.count() == 0:
            print(f"No metadata found for tables: {', '.join(tables_to_append)}")
            return
        
        # Add new columns and transform existing ones
        updated_metadata = filtered_metadata.select(
            col("id"),
            lit("bk_mpo").alias("project_name"),
            col("file_name"),
            col("file_path"),
            current_timestamp().alias("process_date"),
            regexp_extract(col("file_name"), r"\.(\w+)$", 1).alias("file_type"),
            lit("PROCESSED").alias("file_process_state"),  # Assuming all files are processed
            current_timestamp().alias("process_start_dtm"),
            current_timestamp().alias("process_end_dtm"),
            col("file_block_length").alias("file_size")
        )
        
        # Define the target Delta table
        target_table_path = f"{target_metadata_schema}.source_metadata_new"
        
        # Check if the target table exists
        if not spark._jsparkSession.catalog().tableExists(target_table_path):
            raise Exception(f"Target table {target_table_path} does not exist. Please create the table before running this operation.")
        
        # Perform the merge operation
        target_table = DeltaTable.forName(spark, target_table_path)
        
        (target_table.alias("target")
         .merge(
             updated_metadata.alias("updates"),
             "target.id = updates.id"
         )
         .whenMatchedUpdateAll()
         .whenNotMatchedInsertAll()
         .execute())
        
        # Log processing results
        print(f"Metadata updated in {target_table_path}")
        print(f"Number of records processed: {updated_metadata.count()}")
        
    except Exception as e:
        error_message = f"Error updating metadata: {str(e)}\n{traceback.format_exc()}"
        print(error_message)
        raise

# Example usage in main execution
try:
    update_source_metadata(tables_to_append)
    print("Metadata update completed successfully.")
except Exception as e:
    print(f"Failed to update source metadata: {str(e)}")
    dbutils.notebook.exit("FAILURE: Metadata update failed")

print("All operations completed.")
dbutils.notebook.exit("SUCCESS")
