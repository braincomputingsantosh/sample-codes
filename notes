from pyspark.sql import SparkSession
from pyspark.sql.functions import expr
import logging
import time
import threading

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Create widgets for user input
dbutils.widgets.text("source_table", "", "Source Table")
dbutils.widgets.text("target_table", "", "Target Table")
dbutils.widgets.text("where_clause", "", "WHERE Clause (optional)")
dbutils.widgets.text("partition_columns", "", "Partition Columns (comma-separated)")
dbutils.widgets.text("elapsed_time", "0", "Elapsed Time (seconds)")

def update_timer(stop_event):
    start_time = time.time()
    while not stop_event.is_set():
        elapsed_time = int(time.time() - start_time)
        dbutils.widgets.text("elapsed_time", str(elapsed_time))
        time.sleep(1)

def append_data(source_table, target_table, where_clause, partition_columns):
    logger.info(f"Starting append process from {source_table} to {target_table}")
    if where_clause:
        logger.info(f"Applying WHERE clause: {where_clause}")
    if partition_columns:
        logger.info(f"Using partition columns: {partition_columns}")
    
    start_time = time.time()
    
    stop_event = threading.Event()
    timer_thread = threading.Thread(target=update_timer, args=(stop_event,))
    timer_thread.start()
    
    try:
        # Create SparkSession
        spark = SparkSession.builder.appName("AppendData").getOrCreate()
        logger.info("SparkSession created successfully")

        # Read source table
        df = spark.table(source_table)
        logger.info(f"Source table '{source_table}' read successfully")

        # Apply WHERE clause if provided
        if where_clause:
            df = df.filter(expr(where_clause))
            logger.info("WHERE clause applied")

        # Append to target table with partitioning
        write_options = {}
        if partition_columns:
            partition_cols = [col.strip() for col in partition_columns.split(",")]
            write_options["partitionBy"] = partition_cols

        df.write.mode("append").options(**write_options).saveAsTable(target_table)
        logger.info(f"Data appended to target table '{target_table}'")

        # Log some statistics
        row_count = df.count()
        end_time = time.time()
        duration = end_time - start_time
        
        logger.info(f"Append process complete. {row_count} rows from {source_table} have been appended to {target_table}")
        logger.info(f"Append process took {duration:.2f} seconds")

        return True, f"Successfully appended {row_count} rows in {duration:.2f} seconds"

    except Exception as e:
        end_time = time.time()
        duration = end_time - start_time
        error_message = f"An error occurred during append after {duration:.2f} seconds: {str(e)}"
        logger.error(error_message, exc_info=True)
        return False, error_message
    finally:
        stop_event.set()
        timer_thread.join()

# Main execution
source_table = dbutils.widgets.get("source_table")
target_table = dbutils.widgets.get("target_table")
where_clause = dbutils.widgets.get("where_clause")
partition_columns = dbutils.widgets.get("partition_columns")

# Validate inputs
if not all([source_table, target_table]):
    print("Error: Both Source Table and Target Table fields must be filled.")
else:
    # Execute the append function
    success, message = append_data(source_table, target_table, where_clause, partition_columns)
    
    if success:
        print("Append process completed successfully!")
        print(message)
    else:
        print("Append process failed.")
        print(message)

# Clear the elapsed time after completion
dbutils.widgets.remove("elapsed_time")

# Return the result message
dbutils.notebook.exit(message)
