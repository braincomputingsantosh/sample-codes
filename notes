# Databricks on AWS: A Comprehensive Guide

## Introduction

Databricks on AWS is a unified data analytics platform that accelerates innovation by unifying data science, engineering, and business under a collaborative cloud-based environment. Built on top of Apache Sparkâ„¢, Databricks provides a scalable and interactive workspace for big data processing and machine learning.

## Prerequisites

Before deploying Databricks on AWS, ensure you have the following:

- **AWS Account**: An active AWS account with sufficient permissions to create resources.
- **IAM Roles and Permissions**: Proper IAM roles configured for Databricks deployment.
- **Networking Knowledge**: Understanding of AWS VPCs, subnets, and security groups.
- **AWS CLI**: Installed and configured AWS Command Line Interface for advanced setups.
- **Databricks Account**: Access to Databricks or plans to create a new workspace.

## AWS Databricks Resources Overview

When deploying Databricks on AWS, several AWS resources are utilized:

- **Amazon EC2 Instances**: Compute resources for running Databricks clusters.
- **Amazon S3 Buckets**: Storage for data files and logs.
- **IAM Roles and Policies**: Access control and permissions management.
- **Amazon VPC**: Virtual network for resource isolation.
- **Security Groups**: Firewall rules to control inbound and outbound traffic.
- **Elastic IPs**: Optional static IP addresses for consistent access.

## Deployment Guide on Databricks

### Step 1: Set Up AWS Environment

- **Create a VPC**: Set up a Virtual Private Cloud if not already existing.
- **Configure Subnets**: Define public and private subnets across availability zones.
- **Set Up Security Groups**: Establish inbound and outbound rules for network traffic.
- **Create IAM Roles**: Define roles with the necessary permissions for EC2 and S3 access.

### Step 2: Subscribe to Databricks in AWS Marketplace

- Navigate to the [AWS Marketplace](https://aws.amazon.com/marketplace) and search for Databricks.
- **Subscribe**: Accept the terms and conditions to subscribe to the Databricks offering.

### Step 3: Deploy Databricks Workspace

- **Choose Deployment Method**:
  - **Quick Start**: Use the Databricks AWS Quick Start for a simplified setup.
  - **AWS CloudFormation**: Utilize templates for customizable deployments.
- **Specify Parameters**: Input required details such as VPC ID, subnet IDs, and IAM roles.
- **Launch Stack**: Initiate the deployment and wait for the resources to be provisioned.

### Step 4: Access Databricks Workspace

- **Retrieve URL**: After deployment, obtain the workspace URL from the CloudFormation outputs.
- **Login**: Access the Databricks workspace and complete any initial setup steps.
- **Configure Workspace**: Set up users, groups, and cluster policies as needed.

## Accessing Databricks Endpoints in AWS

Accessing Databricks endpoints in AWS involves connecting securely to the Databricks workspace, APIs, and clusters.

### Databricks Workspace URL

- **Workspace URL**: The primary access point for your Databricks environment.
  - Typically in the format: `https://<databricks-instance>.cloud.databricks.com`
- **Retrieve URL**: From the AWS CloudFormation outputs or the Databricks console.

### Databricks REST APIs

- **API Access**: Databricks provides RESTful APIs for programmatic interaction.
- **API Endpoint**: Same as the workspace URL, e.g., `https://<databricks-instance>.cloud.databricks.com/api/2.0/`
- **Authentication**:
  - **Personal Access Tokens**: Generate tokens within the Databricks workspace under User Settings.
  - **AWS Credentials**: Use AWS IAM roles and tokens for authentication.
- **Use Cases**:
  - Automate cluster management.
  - Trigger jobs and workflows.
  - Manage workspace configurations.

### VPC Endpoints (PrivateLink)

- **AWS PrivateLink**: Enables private connectivity between your VPC and Databricks without exposing traffic to the public internet.
- **Benefits**:
  - Enhanced security by keeping traffic within the AWS network.
  - Reduced exposure to public internet risks.
- **Setup**:
  - **Create VPC Endpoint**: In your AWS VPC, create an interface VPC endpoint for Databricks services.
  - **Configure DNS**: Update DNS settings to point to the VPC endpoint.
  - **Security Groups**: Ensure security groups allow necessary inbound and outbound traffic.
- **Databricks Support**: Contact Databricks support for assistance in setting up PrivateLink.

### Cluster Endpoints

- **SSH Access**: Direct SSH access to clusters is not available due to the managed nature of Databricks.
- **Driver and Worker Logs**: Access logs via the Databricks UI or API.
- **Spark UI**: Available through the Databricks workspace for monitoring jobs and performance.

### AWS Console Integration

- **CloudWatch Logs**: Configure Databricks to send logs to AWS CloudWatch for centralized monitoring.
- **S3 Access**: Interact with Amazon S3 buckets for data storage.
  - **Mount S3 Buckets**: Use DBFS (Databricks File System) to mount S3 buckets.
- **IAM Roles**:
  - **Instance Profiles**: Attach IAM roles to clusters for AWS resource access without embedding keys.
  - **Fine-Grained Access Control**: Use IAM policies to control access to specific AWS resources.

## Databricks Task Required Parameters

When creating tasks or jobs in Databricks, certain parameters are required:

- **Job Name**: A unique identifier for the job.
- **Cluster Specification**:
  - **Existing Cluster**: Use a pre-configured cluster.
  - **New Cluster**: Define cluster settings such as instance types and sizes.
- **Notebook or Script**:
  - **Notebook Path**: Specify the path to the Databricks notebook.
  - **Script Location**: Provide the location of the Python/Scala script.
- **Parameters**: Input parameters required by the notebook or script.
- **Schedule**: (Optional) Define a schedule for automated runs.
- **Libraries**: Attach necessary libraries or packages.
- **Alerts**: Configure success or failure notifications.

## Launching Databricks Tasks in AWS

Launching tasks in Databricks involves running jobs that execute notebooks or scripts. You can automate and schedule these tasks using various methods.

### Methods to Launch Tasks

#### Using the Databricks UI

1. **Navigate to Jobs**: In the Databricks workspace, click on the "Jobs" tab.
2. **Create Job**: Click on "Create Job" and fill in the required details.
   - **Task Name**: Provide a name for the task.
   - **Type**: Choose between Notebook, JAR, Python script, etc.
   - **Parameters**: Input any required parameters.
3. **Configure Cluster**: Select an existing cluster or configure a new one.
4. **Schedule**: Set up a schedule if needed.
5. **Submit**: Save and run the job.

#### Using the Databricks CLI

1. **Install CLI**: Ensure the Databricks CLI is installed and configured.
   - Install via `pip install databricks-cli`.
2. **Authenticate**: Use a personal access token to authenticate.
   - Run `databricks configure --token`.
3. **Create Job**: Use the `databricks jobs create` command with a JSON configuration file.
   - Example: `databricks jobs create --json-file job_config.json`.
4. **Run Job**: Use `databricks jobs run-now` to execute the job.
   - Example: `databricks jobs run-now --job-id <job-id>`.

#### Using the Databricks REST API

1. **Endpoint**: Use the `/api/2.0/jobs/` endpoints.
2. **Authentication**: Authenticate using personal access tokens.
3. **Create Job**:
   - Send a POST request to `/api/2.0/jobs/create` with the job configuration in JSON format.
4. **Run Job**:
   - POST to `/api/2.0/jobs/run-now` with the job ID.
5. **Example**:
   ```bash
   curl -X POST https://<databricks-instance>.cloud.databricks.com/api/2.0/jobs/create \
     -H "Authorization: Bearer <your-token>" \
     -d '{
       "name": "Example Job",
       "new_cluster": { ... },
       "notebook_task": {
         "notebook_path": "/Users/your.email@example.com/ExampleNotebook"
       }
     }'
   ```

#### Integrating with AWS Services

- **AWS Lambda**:
  - **Trigger Jobs**: Use Lambda functions to call Databricks APIs and trigger jobs based on events.
  - **Use Cases**: Event-driven data processing, real-time analytics.
- **AWS Step Functions**:
  - **Orchestrate Workflows**: Incorporate Databricks jobs into larger workflows with other AWS services.
  - **Integration**: Use the Step Functions Data Science SDK to create and manage machine learning pipelines.
- **EventBridge (formerly CloudWatch Events)**:
  - **Scheduled Triggers**: Set up rules to trigger Lambda functions or directly invoke Databricks APIs on a schedule.

### AWS-Specific Configurations

- **IAM Roles and Instance Profiles**:
  - **Instance Profiles**: Attach an IAM role to your Databricks cluster to grant permissions to AWS resources like S3, DynamoDB, etc.
  - **Configuration**:
    - Create an IAM role with necessary policies.
    - Attach the role to the cluster via the Databricks workspace UI or API.
- **Network Security**:
  - **Security Groups**: Ensure security groups allow communication between Databricks clusters and AWS services.
  - **VPC Peering**: If accessing resources in another VPC, set up VPC peering.
- **Encryption**:
  - **Data at Rest**: Use AWS KMS keys to encrypt data stored in S3.
  - **Data in Transit**: Databricks encrypts data in transit using TLS.
- **Credential Passthrough**:
  - **ADFS/SAML Integration**: Integrate with AWS Single Sign-On for seamless authentication.

## How to Deploy Databricks Integration Using Service Catalog

AWS Service Catalog enables organizations to create and manage catalogs of IT services approved for use on AWS.

### Step 1: Create a Portfolio

- **Access Service Catalog**: Navigate to the AWS Service Catalog console.
- **Create Portfolio**: Define a new portfolio for Databricks deployments.
  - **Name**: Provide a meaningful name.
  - **Description**: Describe the portfolio's purpose.
  - **Owner**: Specify the owner or administrator.
- **Set Permissions**: Assign IAM users, roles, or groups who can access the portfolio.

### Step 2: Add Databricks Product

- **Define Product**: Create a new product within the portfolio.
- **Upload Template**:
  - Use a CloudFormation template provided by Databricks or create a custom one.
  - Templates define the AWS resources needed for Databricks deployments.
- **Configure Details**:
  - **Product Name**: Name the product (e.g., "Databricks Workspace").
  - **Description**: Provide details about the product.
  - **Distributor**: Optionally, specify the product distributor.

### Step 3: Configure Constraints and Parameters

- **Set Launch Constraints**:
  - **Role-Based Access**: Define IAM roles that users assume when launching the product.
  - **Resource Limits**: Control the size and scope of resources users can provision.
- **Parameter Configuration**:
  - **Default Values**: Set default values for CloudFormation parameters.
  - **Restrictions**: Enforce allowed values or patterns to ensure compliance.

### Step 4: Share Portfolio

- **Share with Accounts**:
  - If operating in a multi-account environment, share the portfolio with other AWS accounts or organizations.
- **Accept Portfolio**:
  - In recipient accounts, accept the shared portfolio to make products available for deployment.

### Step 5: Provision the Product

- **Access Catalog**:
  - Users navigate to the Service Catalog in the AWS console to view available products.
- **Launch Product**:
  - Select the Databricks product and provide necessary parameters during provisioning.
- **Review and Deploy**:
  - Confirm settings, review costs, and launch the deployment.
  - **Provisioned Product**: The deployed instance of the product, managed via Service Catalog.

### Step 6: Manage Deployed Resources

- **Monitor Deployment**:
  - Use the AWS CloudFormation console to monitor stack creation and resource provisioning.
- **Access Workspace**:
  - Once deployed, access the Databricks workspace using the provided URL.
- **Operational Management**:
  - Manage clusters, jobs, libraries, and other resources within Databricks.
- **Updates and Deletion**:
  - Use Service Catalog to update or delete provisioned products, ensuring resources are managed consistently.

---

## Conclusion

Deploying Databricks on AWS provides a scalable and collaborative environment for data analytics and machine learning. By following the prerequisites and deployment steps outlined above, organizations can efficiently set up and manage Databricks workspaces tailored to their needs. Accessing Databricks endpoints securely and integrating with AWS services enables seamless operations. Leveraging AWS Service Catalog further streamlines the deployment process, ensuring compliance and ease of management across the organization.

---

**References**:

- [Databricks Documentation](https://docs.databricks.com/)
- [Databricks REST API Reference](https://docs.databricks.com/dev-tools/api/latest/)
- [Databricks CLI Documentation](https://docs.databricks.com/dev-tools/cli/index.html)
- [AWS Service Catalog Documentation](https://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html)
- [AWS PrivateLink Documentation](https://docs.aws.amazon.com/vpc/latest/privatelink/endpoint-services-overview.html)
- [AWS CloudFormation Templates for Databricks](https://github.com/databricks/databricks-aws-resources)
- [AWS IAM Roles for Databricks](https://docs.databricks.com/administration-guide/cloud-configurations/aws/iam-roles.html)
- [Integrating Databricks with AWS Services](https://docs.databricks.com/integrations/aws/aws-services.html)

---

Feel free to reach out if you have any more questions or need further clarification on any of the steps!
