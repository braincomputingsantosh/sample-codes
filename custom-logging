import psycopg2
import pandas as pd
import yaml
import sys
import time
import logging

# Configure logging
logging.basicConfig(
    filename='update_log.log',  # Log file name
    level=logging.INFO,         # Log level
    format='%(asctime)s - %(levelname)s - %(message)s',  # Log format
    datefmt='%Y-%m-%d %H:%M:%S',  # Date format
)

# Load states from YAML file
with open("config.yaml", 'r') as file:
    config = yaml.safe_load(file)
states_to_update = config['states_to_update']

# Connect to your Citus CStore database
conn = psycopg2.connect("dbname=yourdbname user=yourusername password=yourpassword")
cur = conn.cursor()

batch_size = 100000
total_updates = 0

# Process each state
for state in states_to_update:
    logging.info(f"Processing state: {state}")
    error_count = 0
    last_processed_logrecno = 0  # Reset for each state
    state_updates = 0

    while True:
        try:
            # Start timing the batch update
            start_time = time.time()

            # Fetch a batch of records using state and logrecno
            query = """
                WITH indexed_dd AS (
                    SELECT 
                        segment, 
                        tbl, 
                        field_code,
                        ROW_NUMBER() OVER (PARTITION BY segment, tbl ORDER BY sort_id) - 1 AS index_position
                    FROM census.dd_seq_dhc
                    WHERE field_code IS NOT NULL
                ),
                to_update AS (
                    SELECT sf1.state, sf1.logrecno, sf1.segment, sf1.tbl, dd.field_code
                    FROM census.sf1_temp sf1
                    JOIN indexed_dd dd
                    ON sf1.segment::text = dd.segment::text
                    AND UPPER(SPLIT_PART(sf1.tbl, '-', 1)) = UPPER(dd.tbl)
                    AND CAST(SPLIT_PART(sf1.tbl, '-', 2) AS INTEGER) = dd.index_position
                    WHERE sf1.state = %s AND sf1.logrecno > %s
                    AND sf1.tbl != dd.field_code  -- Only select rows where tbl needs to be updated
                    ORDER BY sf1.logrecno
                    LIMIT %s
                )
                SELECT * FROM to_update
            """
            df = pd.read_sql(query, conn, params=(state, last_processed_logrecno, batch_size))
            
            if df.empty:
                logging.info(f"Completed processing for state {state}. Total updates: {state_updates}")
                break
            
            # Convert the DataFrame to native Python types for psycopg2
            states = df['state'].astype(str).to_list()
            logrecnos = df['logrecno'].astype(int).to_list()
            field_codes = df['field_code'].astype(str).to_list()
            
            # Perform bulk updates
            update_query = """
                UPDATE census.sf1_temp
                SET tbl = %s
                WHERE state = %s AND logrecno = %s
            """
            update_data = list(zip(field_codes, states, logrecnos))
            cur.executemany(update_query, update_data)
            
            # Update counters
            state_updates += len(update_data)
            total_updates += len(update_data)
            
            # Update the last processed logrecno
            last_processed_logrecno = logrecnos[-1]
            
            # Commit the updates
            conn.commit()
            
            # End timing the batch update
            end_time = time.time()
            elapsed_time = end_time - start_time
            
            logging.info(f"Processed batch for state {state}. Updated {len(update_data)} records. "
                         f"Current logrecno: {last_processed_logrecno}. "
                         f"Batch time: {elapsed_time:.2f} seconds")
            
        except Exception as e:
            logging.error(f"Error processing state {state}: {e}")
            conn.rollback()
            error_count += 1
            if error_count > 10:
                logging.error(f"Too many errors in state {state}, skipping to next state.")
                break

logging.info(f"All updates completed. Total records updated: {total_updates}")
cur.close()
conn.close()
logging.info("Database connection closed. Exiting program.")
sys.exit(0)
